{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed22d84",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<br>\n",
    "\n",
    "<h1>A1: Classification Model Development</h1>\n",
    "<i>Computational Data Analytics with Python - DAT-5390 - Assignment Submission</i>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Written by Sanskriti Garg - Student of Master of Business Analytics<br>\n",
    "Hult International Business School, San Francisco<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "In this assignment, we have developed a model to predict whether cross selling will be a success for a customer or not. \n",
    "\n",
    "<h3>Methodology</h3><br>\n",
    "\n",
    "We start by exploring the dataset and understanding the data type, data format and existence of any missing values in the set.After that we divide the data into different data types (continuous, interval and categorical). We then log transform current feature, engineer new features and understand feature importance. We understand feature importance through logistic regression in statsmodel where we look at the features with significant p values and we also plot the feature importance plot as shared in the class. Basis these two, we shortlist features to try and create 4 sets potential explanatory variables for our future models. \n",
    "\n",
    "We use the following models to test the four sets of variables and derive the training score, test score, train-test gap and AUC score.  \n",
    "<br>\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Classifier\n",
    "3. Random Forest Classifier\n",
    "4. Gradient Boosting Classifier\n",
    "\n",
    "The final model is chosen basis the AUC score and train-test gap and then we go on and perform hyperparameter tuning on our final model. In our case there were three models which had nearby AUC score, hence we have taken these models for hyperparameter tuning.\n",
    "\n",
    "After hyperparameter tuning, we have also plotted the confusion matrix for the final model. \n",
    "\n",
    "The final model that has qualified for the assignment is Decision Tree Classifier. \n",
    "\n",
    "We have detailed out the confusion matrix and the final model results in the end\n",
    "<h3>Response Variable</h3><br>\n",
    "\n",
    "The response variable is Cross_Sell_Success which is discrete data. \n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd2f501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>PC_LOGINS</th>\n",
       "      <th>MOBILE_LOGINS</th>\n",
       "      <th>WEEKLY_PLAN</th>\n",
       "      <th>LATE_DELIVERIES</th>\n",
       "      <th>AVG_PREP_VID_TIME</th>\n",
       "      <th>LARGEST_ORDER_SIZE</th>\n",
       "      <th>AVG_MEAN_RATING</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>steffon.baratheon@yahoo.com</td>\n",
       "      <td>4920.0</td>\n",
       "      <td>493</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>265.6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137.41</td>\n",
       "      <td>6</td>\n",
       "      <td>2.894737</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>harlon.greyjoy@visa.com</td>\n",
       "      <td>6150.0</td>\n",
       "      <td>361</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.20</td>\n",
       "      <td>5</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>monster@protonmail.com</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>278</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>164.4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>127.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>damon.lannister.(lord)@yahoo.com</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>269</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>176.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>129.78</td>\n",
       "      <td>6</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>raynald.westerling@jnj.com</td>\n",
       "      <td>3427.5</td>\n",
       "      <td>276</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>164.6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>34.42</td>\n",
       "      <td>3</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CROSS_SELL_SUCCESS                             EMAIL  REVENUE  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  CANCELLATIONS_AFTER_NOON  PC_LOGINS  MOBILE_LOGINS  WEEKLY_PLAN  LATE_DELIVERIES   AVG_PREP_VID_TIME  LARGEST_ORDER_SIZE  AVG_MEAN_RATING  TOTAL_PHOTOS_VIEWED\n",
       "0                   1       steffon.baratheon@yahoo.com   4920.0                  493                   9                            1                         10                    265.6                         5          5              2            0                 0             137.41                   6         2.894737                  456\n",
       "1                   0           harlon.greyjoy@visa.com   6150.0                  361                   9                            1                          6                    247.0                         2          5              1            0                 0             120.20                   5         2.631579                  680\n",
       "2                   0            monster@protonmail.com   3435.0                  278                   6                            1                          4                    164.4                         0          6              1            5                 0             127.00                   3         3.684211                  145\n",
       "3                   1  damon.lannister.(lord)@yahoo.com   3330.0                  269                   8                            1                          2                    176.0                         5          5              2            0                 0             129.78                   6         3.157895                  418\n",
       "4                   1        raynald.westerling@jnj.com   3427.5                  276                   7                            1                         10                    164.6                         0          6              1           14                 0              34.42                   3         3.157895                  174"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas            as pd                          # data science essentials\n",
    "import matplotlib.pyplot as plt                         # data visualization\n",
    "import seaborn           as sns                         # enhanced data viz\n",
    "import numpy             as np                          # numpy array \n",
    "from sklearn.model_selection import train_test_split    # train-test split\n",
    "from sklearn.linear_model import LogisticRegression     # logistic regression\n",
    "import statsmodels.formula.api as smf                   # logistic regression\n",
    "from sklearn.metrics import confusion_matrix            # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score               # auc score\n",
    "from sklearn.preprocessing import StandardScaler        # standard scaler\n",
    "from sklearn.tree import DecisionTreeClassifier         # classification trees\n",
    "from sklearn.tree import plot_tree                      # tree plots\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV  # hyperparameter tuning\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gradient boosting classifier\n",
    "\n",
    "# loading data\n",
    "cross_sell = pd.read_excel(io = 'Cross_Sell_Success_Dataset_2023.xlsx' )\n",
    "\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "# displaying the head of the dataset\n",
    "cross_sell.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485b123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1946 entries, 0 to 1945\n",
      "Data columns (total 17 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   CROSS_SELL_SUCCESS           1946 non-null   int64  \n",
      " 1   EMAIL                        1946 non-null   object \n",
      " 2   REVENUE                      1946 non-null   float64\n",
      " 3   TOTAL_MEALS_ORDERED          1946 non-null   int64  \n",
      " 4   UNIQUE_MEALS_PURCH           1946 non-null   int64  \n",
      " 5   CONTACTS_W_CUSTOMER_SERVICE  1946 non-null   int64  \n",
      " 6   PRODUCT_CATEGORIES_VIEWED    1946 non-null   int64  \n",
      " 7   AVG_TIME_PER_SITE_VISIT      1946 non-null   float64\n",
      " 8   CANCELLATIONS_AFTER_NOON     1946 non-null   int64  \n",
      " 9   PC_LOGINS                    1946 non-null   int64  \n",
      " 10  MOBILE_LOGINS                1946 non-null   int64  \n",
      " 11  WEEKLY_PLAN                  1946 non-null   int64  \n",
      " 12  LATE_DELIVERIES              1946 non-null   int64  \n",
      " 13  AVG_PREP_VID_TIME            1946 non-null   float64\n",
      " 14  LARGEST_ORDER_SIZE           1946 non-null   int64  \n",
      " 15  AVG_MEAN_RATING              1946 non-null   float64\n",
      " 16  TOTAL_PHOTOS_VIEWED          1946 non-null   int64  \n",
      "dtypes: float64(4), int64(12), object(1)\n",
      "memory usage: 258.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# INFOrmation about each variable\n",
    "cross_sell.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf76ba51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CROSS_SELL_SUCCESS             0\n",
       "EMAIL                          0\n",
       "REVENUE                        0\n",
       "TOTAL_MEALS_ORDERED            0\n",
       "UNIQUE_MEALS_PURCH             0\n",
       "CONTACTS_W_CUSTOMER_SERVICE    0\n",
       "PRODUCT_CATEGORIES_VIEWED      0\n",
       "AVG_TIME_PER_SITE_VISIT        0\n",
       "CANCELLATIONS_AFTER_NOON       0\n",
       "PC_LOGINS                      0\n",
       "MOBILE_LOGINS                  0\n",
       "WEEKLY_PLAN                    0\n",
       "LATE_DELIVERIES                0\n",
       "AVG_PREP_VID_TIME              0\n",
       "LARGEST_ORDER_SIZE             0\n",
       "AVG_MEAN_RATING                0\n",
       "TOTAL_PHOTOS_VIEWED            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "cross_sell.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31653732",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# visual_cm\n",
    "########################################\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a34b83",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<h3>Feature Engineering: Developing New Features</h3>\n",
    "<br>\n",
    "\n",
    "<b>a. Correcting incorrectly labeled columns<br>\n",
    "\n",
    "Two columns have been incorrectly labeled. Below we have corrected the errors in the column name labels. \n",
    "\n",
    "1. LATE_DELIVERIES column has a space in the error leading to error upon being called\n",
    "2. LARGEST_ORDER_SIZE is actually the average meals ordered. \n",
    "\n",
    "For above we have created two new columns with correct labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdad5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1946 entries, 0 to 1945\n",
      "Data columns (total 19 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   CROSS_SELL_SUCCESS           1946 non-null   int64  \n",
      " 1   EMAIL                        1946 non-null   object \n",
      " 2   REVENUE                      1946 non-null   float64\n",
      " 3   TOTAL_MEALS_ORDERED          1946 non-null   int64  \n",
      " 4   UNIQUE_MEALS_PURCH           1946 non-null   int64  \n",
      " 5   CONTACTS_W_CUSTOMER_SERVICE  1946 non-null   int64  \n",
      " 6   PRODUCT_CATEGORIES_VIEWED    1946 non-null   int64  \n",
      " 7   AVG_TIME_PER_SITE_VISIT      1946 non-null   float64\n",
      " 8   CANCELLATIONS_AFTER_NOON     1946 non-null   int64  \n",
      " 9   PC_LOGINS                    1946 non-null   int64  \n",
      " 10  MOBILE_LOGINS                1946 non-null   int64  \n",
      " 11  WEEKLY_PLAN                  1946 non-null   int64  \n",
      " 12  LATE_DELIVERIES              1946 non-null   int64  \n",
      " 13  AVG_PREP_VID_TIME            1946 non-null   float64\n",
      " 14  LARGEST_ORDER_SIZE           1946 non-null   int64  \n",
      " 15  AVG_MEAN_RATING              1946 non-null   float64\n",
      " 16  TOTAL_PHOTOS_VIEWED          1946 non-null   int64  \n",
      " 17  c_LATE_DELIVERIES            1946 non-null   int64  \n",
      " 18  c_AVG_MEALS_ORDERED          1946 non-null   int64  \n",
      "dtypes: float64(4), int64(14), object(1)\n",
      "memory usage: 289.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Correcting erroneous columns \n",
    "# 1. Removing space from the column name 'LATE_DELIVERIES '\n",
    "# 2. Correcting Column name 'LARGEST_ORDER_SIZE' to ' AVG_MEALS_ORDERED'\n",
    "\n",
    "cross_sell['c_LATE_DELIVERIES']   = cross_sell['LATE_DELIVERIES ']\n",
    "cross_sell['c_AVG_MEALS_ORDERED'] = cross_sell['LARGEST_ORDER_SIZE']\n",
    "\n",
    "cross_sell.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdf77b",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>b. Log transformation</b><br><br>\n",
    "Log transforming features with skewness more than 1 or less than -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee68a029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1946 entries, 0 to 1945\n",
      "Data columns (total 26 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   CROSS_SELL_SUCCESS            1946 non-null   int64  \n",
      " 1   EMAIL                         1946 non-null   object \n",
      " 2   REVENUE                       1946 non-null   float64\n",
      " 3   TOTAL_MEALS_ORDERED           1946 non-null   int64  \n",
      " 4   UNIQUE_MEALS_PURCH            1946 non-null   int64  \n",
      " 5   CONTACTS_W_CUSTOMER_SERVICE   1946 non-null   int64  \n",
      " 6   PRODUCT_CATEGORIES_VIEWED     1946 non-null   int64  \n",
      " 7   AVG_TIME_PER_SITE_VISIT       1946 non-null   float64\n",
      " 8   CANCELLATIONS_AFTER_NOON      1946 non-null   int64  \n",
      " 9   PC_LOGINS                     1946 non-null   int64  \n",
      " 10  MOBILE_LOGINS                 1946 non-null   int64  \n",
      " 11  WEEKLY_PLAN                   1946 non-null   int64  \n",
      " 12  LATE_DELIVERIES               1946 non-null   int64  \n",
      " 13  AVG_PREP_VID_TIME             1946 non-null   float64\n",
      " 14  LARGEST_ORDER_SIZE            1946 non-null   int64  \n",
      " 15  AVG_MEAN_RATING               1946 non-null   float64\n",
      " 16  TOTAL_PHOTOS_VIEWED           1946 non-null   int64  \n",
      " 17  c_LATE_DELIVERIES             1946 non-null   int64  \n",
      " 18  c_AVG_MEALS_ORDERED           1946 non-null   int64  \n",
      " 19  LOG_TOTAL_MEALS_ORDERED       1946 non-null   float64\n",
      " 20  LOG_AVG_TIME_PER_SITE_VISIT   1946 non-null   float64\n",
      " 21  LOG_CANCELLATIONS_AFTER_NOON  1946 non-null   float64\n",
      " 22  LOG_WEEKLY_PLAN               1946 non-null   float64\n",
      " 23  LOG_c_LATE_DELIVERIES         1946 non-null   float64\n",
      " 24  LOG_AVG_PREP_VID_TIME         1946 non-null   float64\n",
      " 25  LOG_TOTAL_PHOTOS_VIEWED       1946 non-null   float64\n",
      "dtypes: float64(11), int64(14), object(1)\n",
      "memory usage: 395.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# log transforming features with skewness more than 1 or less than -1\n",
    "cross_sell['LOG_TOTAL_MEALS_ORDERED'] = np.log(cross_sell['TOTAL_MEALS_ORDERED'] + 0.001)\n",
    "cross_sell['LOG_AVG_TIME_PER_SITE_VISIT'] = np.log(cross_sell['AVG_TIME_PER_SITE_VISIT'] + 0.001)\n",
    "cross_sell['LOG_CANCELLATIONS_AFTER_NOON'] = np.log(cross_sell['CANCELLATIONS_AFTER_NOON']+ 0.001)\n",
    "cross_sell['LOG_WEEKLY_PLAN'] = np.log(cross_sell['WEEKLY_PLAN'] + 0.001)\n",
    "cross_sell['LOG_c_LATE_DELIVERIES'] = np.log(cross_sell['c_LATE_DELIVERIES']+ 0.001)\n",
    "cross_sell['LOG_AVG_PREP_VID_TIME'] = np.log(cross_sell['AVG_PREP_VID_TIME'] + 0.001)\n",
    "cross_sell['LOG_TOTAL_PHOTOS_VIEWED'] = np.log(cross_sell['TOTAL_PHOTOS_VIEWED'] + 0.001)\n",
    "\n",
    "# Displaying INFOrmation with updated columns\n",
    "cross_sell.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeceff8",
   "metadata": {},
   "source": [
    "<b>c. Contacts with customer service</b><br><br>\n",
    "We also try classifying contacts with customer service. \n",
    "\n",
    "If the contacts with customer service are less than 10 we say that these are low contacts and assign a 0 and if the contacts are >10 we say high contacts and assign them a 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d38f1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONT_W_CS_CAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CONT_W_CS_CAT\n",
       "0              0\n",
       "1              0\n",
       "2              0\n",
       "3              0\n",
       "4              0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering Contacts with customer service \n",
    "\n",
    "# placeholder variables\n",
    "cross_sell['CONT_W_CS_CAT'] = 0\n",
    "\n",
    "for index, value in cross_sell.iterrows():\n",
    "    \n",
    "\n",
    "    # Alloting 0 if CONTACTS_W_CUSTOMER_SERVICE <10\n",
    "    if cross_sell.loc[index, 'CONTACTS_W_CUSTOMER_SERVICE'] < 10:\n",
    "        cross_sell.loc[index, 'CONT_W_CS_CAT'] = 0\n",
    "        \n",
    "    else:\n",
    "        # Alotting 1 if CONTACTS_W_CUSTOMER_SERVICE >=10\n",
    "        cross_sell.loc[index, 'CONT_W_CS_CAT'] = 1\n",
    "        \n",
    "# checking results\n",
    "cross_sell[  ['CONT_W_CS_CAT']  ].head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240f3b1",
   "metadata": {},
   "source": [
    "<b>d. Total Logins (TOTAL_LOGINS)<br></b>\n",
    "\n",
    "We have combined the PC logins and Mobile logins to arrive at the Total Logins. \n",
    "\n",
    "<b>e. Cancel Proportion (CANCEL_PROP)<br></b>\n",
    "\n",
    "This feature shows meals canceled as a proportion of total meals ordered. The assumption is that higher the percentage of cancellation proportion, lesser be success of cross sell promotion (Y-variable). \n",
    "\n",
    "<b>f. Photos viewed per total login (PHOTO_PER_LOGIN)</b>\n",
    "\n",
    "We have tried to find the Photos per Total login which was calculated above. The assumption here is that higher the number of photos per login, more will be the interest of the customer in the offerings and hence more will be the likelihood of trying the cross sell promotion(Y-variable).\n",
    "\n",
    "<b>g. Product categories viewed per total logins (PRO_CAT_LOGIN)</b>\n",
    "\n",
    "Here we find the proportion of product categories viewed per total logins. The assumption here is if the number of categories per login is high, then the user might be interested more in the offering and hence the chances of cross sell promotion will be higher (Y-variable). <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40630b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering new features \n",
    "\n",
    "cross_sell['TOTAL_LOGINS']     = cross_sell['PC_LOGINS'] + cross_sell['MOBILE_LOGINS']\n",
    "\n",
    "cross_sell['CANCEL_PROP']      = cross_sell['CANCELLATIONS_AFTER_NOON'] / cross_sell['TOTAL_MEALS_ORDERED']\n",
    "cross_sell['PHOTO_PER_LOGIN']  = cross_sell['TOTAL_PHOTOS_VIEWED'] / cross_sell['TOTAL_LOGINS']\n",
    "cross_sell['PRO_CAT_LOGIN']    = cross_sell['PRODUCT_CATEGORIES_VIEWED'] / cross_sell['TOTAL_LOGINS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3872c",
   "metadata": {},
   "source": [
    "<b>h. Unique meals as a proportion of total meals ordered</b><br><br>\n",
    "We want to see if a customer tries more unique meals does he/she be more interested in the wine promotion as they might be finding the Apprentice chef interesting and hence want to try its various offering. We are looking at the proportion of unique meals to total meals ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a687f43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>PC_LOGINS</th>\n",
       "      <th>MOBILE_LOGINS</th>\n",
       "      <th>WEEKLY_PLAN</th>\n",
       "      <th>LATE_DELIVERIES</th>\n",
       "      <th>AVG_PREP_VID_TIME</th>\n",
       "      <th>LARGEST_ORDER_SIZE</th>\n",
       "      <th>AVG_MEAN_RATING</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "      <th>c_LATE_DELIVERIES</th>\n",
       "      <th>c_AVG_MEALS_ORDERED</th>\n",
       "      <th>LOG_TOTAL_MEALS_ORDERED</th>\n",
       "      <th>LOG_AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>LOG_CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>LOG_WEEKLY_PLAN</th>\n",
       "      <th>LOG_c_LATE_DELIVERIES</th>\n",
       "      <th>LOG_AVG_PREP_VID_TIME</th>\n",
       "      <th>LOG_TOTAL_PHOTOS_VIEWED</th>\n",
       "      <th>CONT_W_CS_CAT</th>\n",
       "      <th>TOTAL_LOGINS</th>\n",
       "      <th>CANCEL_PROP</th>\n",
       "      <th>PHOTO_PER_LOGIN</th>\n",
       "      <th>PRO_CAT_LOGIN</th>\n",
       "      <th>UNI_PER_TOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>steffon.baratheon@yahoo.com</td>\n",
       "      <td>4920.0</td>\n",
       "      <td>493</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>265.6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137.41</td>\n",
       "      <td>6</td>\n",
       "      <td>2.894737</td>\n",
       "      <td>456</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.200511</td>\n",
       "      <td>5.581995</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.922976</td>\n",
       "      <td>6.122495</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.010142</td>\n",
       "      <td>65.142857</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.018256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>harlon.greyjoy@visa.com</td>\n",
       "      <td>6150.0</td>\n",
       "      <td>361</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.20</td>\n",
       "      <td>5</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.888881</td>\n",
       "      <td>5.509392</td>\n",
       "      <td>0.693647</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.789165</td>\n",
       "      <td>6.522094</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>113.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>monster@protonmail.com</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>278</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>164.4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>127.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.627625</td>\n",
       "      <td>5.102309</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.844195</td>\n",
       "      <td>4.976741</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.714286</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.021583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>damon.lannister.(lord)@yahoo.com</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>269</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>176.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>129.78</td>\n",
       "      <td>6</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>418</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.594715</td>\n",
       "      <td>5.170490</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.865848</td>\n",
       "      <td>6.035484</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.018587</td>\n",
       "      <td>59.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.029740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>raynald.westerling@jnj.com</td>\n",
       "      <td>3427.5</td>\n",
       "      <td>276</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>164.6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>34.42</td>\n",
       "      <td>3</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.620404</td>\n",
       "      <td>5.103524</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>2.639129</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>3.538667</td>\n",
       "      <td>5.159061</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.857143</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.025362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CROSS_SELL_SUCCESS                             EMAIL  REVENUE  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  CANCELLATIONS_AFTER_NOON  PC_LOGINS  MOBILE_LOGINS  WEEKLY_PLAN  LATE_DELIVERIES   AVG_PREP_VID_TIME  LARGEST_ORDER_SIZE  AVG_MEAN_RATING  TOTAL_PHOTOS_VIEWED  c_LATE_DELIVERIES  c_AVG_MEALS_ORDERED  LOG_TOTAL_MEALS_ORDERED  LOG_AVG_TIME_PER_SITE_VISIT  LOG_CANCELLATIONS_AFTER_NOON  LOG_WEEKLY_PLAN  LOG_c_LATE_DELIVERIES  LOG_AVG_PREP_VID_TIME  LOG_TOTAL_PHOTOS_VIEWED  CONT_W_CS_CAT  TOTAL_LOGINS  CANCEL_PROP  PHOTO_PER_LOGIN  PRO_CAT_LOGIN  UNI_PER_TOT\n",
       "0                   1       steffon.baratheon@yahoo.com   4920.0                  493                   9                            1                         10                    265.6                         5          5              2            0                 0             137.41                   6         2.894737                  456                  0                    6                 6.200511                     5.581995                      1.609638        -6.907755              -6.907755               4.922976                 6.122495              0             7     0.010142        65.142857       1.428571     0.018256\n",
       "1                   0           harlon.greyjoy@visa.com   6150.0                  361                   9                            1                          6                    247.0                         2          5              1            0                 0             120.20                   5         2.631579                  680                  0                    5                 5.888881                     5.509392                      0.693647        -6.907755              -6.907755               4.789165                 6.522094              0             6     0.005540       113.333333       1.000000     0.024931\n",
       "2                   0            monster@protonmail.com   3435.0                  278                   6                            1                          4                    164.4                         0          6              1            5                 0             127.00                   3         3.684211                  145                  0                    3                 5.627625                     5.102309                     -6.907755         1.609638              -6.907755               4.844195                 4.976741              0             7     0.000000        20.714286       0.571429     0.021583\n",
       "3                   1  damon.lannister.(lord)@yahoo.com   3330.0                  269                   8                            1                          2                    176.0                         5          5              2            0                 0             129.78                   6         3.157895                  418                  0                    6                 5.594715                     5.170490                      1.609638        -6.907755              -6.907755               4.865848                 6.035484              0             7     0.018587        59.714286       0.285714     0.029740\n",
       "4                   1        raynald.westerling@jnj.com   3427.5                  276                   7                            1                         10                    164.6                         0          6              1           14                 0              34.42                   3         3.157895                  174                  0                    3                 5.620404                     5.103524                     -6.907755         2.639129              -6.907755               3.538667                 5.159061              0             7     0.000000        24.857143       1.428571     0.025362"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique meals to total meals purchased\n",
    "cross_sell['UNI_PER_TOT'] = cross_sell['UNIQUE_MEALS_PURCH']/cross_sell['TOTAL_MEALS_ORDERED']\n",
    "\n",
    "\n",
    "cross_sell.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ca3b0",
   "metadata": {},
   "source": [
    "<br><b>i. Late deliveries per total meals ordered</b><br><br>\n",
    "\n",
    "\n",
    "Here we want to see if the late deliveries lead to the customer interest going down in any promotion offered by Apprentice chef. Hence, we look at late deliveries as proportion of total meals ordered to see if the higher late delivery proportion lead to a lower success rate of the cross promotion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb6cfc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>PC_LOGINS</th>\n",
       "      <th>MOBILE_LOGINS</th>\n",
       "      <th>WEEKLY_PLAN</th>\n",
       "      <th>LATE_DELIVERIES</th>\n",
       "      <th>AVG_PREP_VID_TIME</th>\n",
       "      <th>LARGEST_ORDER_SIZE</th>\n",
       "      <th>AVG_MEAN_RATING</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "      <th>c_LATE_DELIVERIES</th>\n",
       "      <th>c_AVG_MEALS_ORDERED</th>\n",
       "      <th>LOG_TOTAL_MEALS_ORDERED</th>\n",
       "      <th>LOG_AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>LOG_CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>LOG_WEEKLY_PLAN</th>\n",
       "      <th>LOG_c_LATE_DELIVERIES</th>\n",
       "      <th>LOG_AVG_PREP_VID_TIME</th>\n",
       "      <th>LOG_TOTAL_PHOTOS_VIEWED</th>\n",
       "      <th>CONT_W_CS_CAT</th>\n",
       "      <th>TOTAL_LOGINS</th>\n",
       "      <th>CANCEL_PROP</th>\n",
       "      <th>PHOTO_PER_LOGIN</th>\n",
       "      <th>PRO_CAT_LOGIN</th>\n",
       "      <th>UNI_PER_TOT</th>\n",
       "      <th>LAT_DEL_PER_TOTAL_MEALS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>steffon.baratheon@yahoo.com</td>\n",
       "      <td>4920.0</td>\n",
       "      <td>493</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>265.6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137.41</td>\n",
       "      <td>6</td>\n",
       "      <td>2.894737</td>\n",
       "      <td>456</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.200511</td>\n",
       "      <td>5.581995</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.922976</td>\n",
       "      <td>6.122495</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.010142</td>\n",
       "      <td>65.142857</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>harlon.greyjoy@visa.com</td>\n",
       "      <td>6150.0</td>\n",
       "      <td>361</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.20</td>\n",
       "      <td>5</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.888881</td>\n",
       "      <td>5.509392</td>\n",
       "      <td>0.693647</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.789165</td>\n",
       "      <td>6.522094</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>113.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024931</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>monster@protonmail.com</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>278</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>164.4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>127.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.627625</td>\n",
       "      <td>5.102309</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.844195</td>\n",
       "      <td>4.976741</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.714286</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.021583</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>damon.lannister.(lord)@yahoo.com</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>269</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>176.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>129.78</td>\n",
       "      <td>6</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>418</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.594715</td>\n",
       "      <td>5.170490</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.865848</td>\n",
       "      <td>6.035484</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.018587</td>\n",
       "      <td>59.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>raynald.westerling@jnj.com</td>\n",
       "      <td>3427.5</td>\n",
       "      <td>276</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>164.6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>34.42</td>\n",
       "      <td>3</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.620404</td>\n",
       "      <td>5.103524</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>2.639129</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>3.538667</td>\n",
       "      <td>5.159061</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.857143</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.025362</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CROSS_SELL_SUCCESS                             EMAIL  REVENUE  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  CANCELLATIONS_AFTER_NOON  PC_LOGINS  MOBILE_LOGINS  WEEKLY_PLAN  LATE_DELIVERIES   AVG_PREP_VID_TIME  LARGEST_ORDER_SIZE  AVG_MEAN_RATING  TOTAL_PHOTOS_VIEWED  c_LATE_DELIVERIES  c_AVG_MEALS_ORDERED  LOG_TOTAL_MEALS_ORDERED  LOG_AVG_TIME_PER_SITE_VISIT  LOG_CANCELLATIONS_AFTER_NOON  LOG_WEEKLY_PLAN  LOG_c_LATE_DELIVERIES  LOG_AVG_PREP_VID_TIME  LOG_TOTAL_PHOTOS_VIEWED  CONT_W_CS_CAT  TOTAL_LOGINS  CANCEL_PROP  PHOTO_PER_LOGIN  PRO_CAT_LOGIN  UNI_PER_TOT  LAT_DEL_PER_TOTAL_MEALS\n",
       "0                   1       steffon.baratheon@yahoo.com   4920.0                  493                   9                            1                         10                    265.6                         5          5              2            0                 0             137.41                   6         2.894737                  456                  0                    6                 6.200511                     5.581995                      1.609638        -6.907755              -6.907755               4.922976                 6.122495              0             7     0.010142        65.142857       1.428571     0.018256                      0.0\n",
       "1                   0           harlon.greyjoy@visa.com   6150.0                  361                   9                            1                          6                    247.0                         2          5              1            0                 0             120.20                   5         2.631579                  680                  0                    5                 5.888881                     5.509392                      0.693647        -6.907755              -6.907755               4.789165                 6.522094              0             6     0.005540       113.333333       1.000000     0.024931                      0.0\n",
       "2                   0            monster@protonmail.com   3435.0                  278                   6                            1                          4                    164.4                         0          6              1            5                 0             127.00                   3         3.684211                  145                  0                    3                 5.627625                     5.102309                     -6.907755         1.609638              -6.907755               4.844195                 4.976741              0             7     0.000000        20.714286       0.571429     0.021583                      0.0\n",
       "3                   1  damon.lannister.(lord)@yahoo.com   3330.0                  269                   8                            1                          2                    176.0                         5          5              2            0                 0             129.78                   6         3.157895                  418                  0                    6                 5.594715                     5.170490                      1.609638        -6.907755              -6.907755               4.865848                 6.035484              0             7     0.018587        59.714286       0.285714     0.029740                      0.0\n",
       "4                   1        raynald.westerling@jnj.com   3427.5                  276                   7                            1                         10                    164.6                         0          6              1           14                 0              34.42                   3         3.157895                  174                  0                    3                 5.620404                     5.103524                     -6.907755         2.639129              -6.907755               3.538667                 5.159061              0             7     0.000000        24.857143       1.428571     0.025362                      0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# late deliveries proportion to total meals ordered\n",
    "cross_sell['LAT_DEL_PER_TOTAL_MEALS'] = cross_sell ['c_LATE_DELIVERIES'] / cross_sell['TOTAL_MEALS_ORDERED']\n",
    "\n",
    "cross_sell.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a715a51",
   "metadata": {},
   "source": [
    "<b>j. Email Classification</b><br><br>\n",
    "\n",
    "We have categorized emails basis the domain name under junk, personal and professional mails. We then do one_hot encoding to get the data ready for our model which is make the categorical data into discrete data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363ba55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>PC_LOGINS</th>\n",
       "      <th>MOBILE_LOGINS</th>\n",
       "      <th>WEEKLY_PLAN</th>\n",
       "      <th>LATE_DELIVERIES</th>\n",
       "      <th>AVG_PREP_VID_TIME</th>\n",
       "      <th>LARGEST_ORDER_SIZE</th>\n",
       "      <th>AVG_MEAN_RATING</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "      <th>c_LATE_DELIVERIES</th>\n",
       "      <th>c_AVG_MEALS_ORDERED</th>\n",
       "      <th>LOG_TOTAL_MEALS_ORDERED</th>\n",
       "      <th>LOG_AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>LOG_CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>LOG_WEEKLY_PLAN</th>\n",
       "      <th>LOG_c_LATE_DELIVERIES</th>\n",
       "      <th>LOG_AVG_PREP_VID_TIME</th>\n",
       "      <th>LOG_TOTAL_PHOTOS_VIEWED</th>\n",
       "      <th>CONT_W_CS_CAT</th>\n",
       "      <th>TOTAL_LOGINS</th>\n",
       "      <th>CANCEL_PROP</th>\n",
       "      <th>PHOTO_PER_LOGIN</th>\n",
       "      <th>PRO_CAT_LOGIN</th>\n",
       "      <th>UNI_PER_TOT</th>\n",
       "      <th>LAT_DEL_PER_TOTAL_MEALS</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>EMAIL_CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>steffon.baratheon@yahoo.com</td>\n",
       "      <td>4920.0</td>\n",
       "      <td>493</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>265.6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137.41</td>\n",
       "      <td>6</td>\n",
       "      <td>2.894737</td>\n",
       "      <td>456</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.200511</td>\n",
       "      <td>5.581995</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.922976</td>\n",
       "      <td>6.122495</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.010142</td>\n",
       "      <td>65.142857</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>PERSONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>harlon.greyjoy@visa.com</td>\n",
       "      <td>6150.0</td>\n",
       "      <td>361</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.20</td>\n",
       "      <td>5</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.888881</td>\n",
       "      <td>5.509392</td>\n",
       "      <td>0.693647</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.789165</td>\n",
       "      <td>6.522094</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>113.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>visa.com</td>\n",
       "      <td>PROFESSIONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>monster@protonmail.com</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>278</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>164.4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>127.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.627625</td>\n",
       "      <td>5.102309</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.844195</td>\n",
       "      <td>4.976741</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.714286</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.021583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>protonmail.com</td>\n",
       "      <td>PERSONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>damon.lannister.(lord)@yahoo.com</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>269</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>176.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>129.78</td>\n",
       "      <td>6</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>418</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.594715</td>\n",
       "      <td>5.170490</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.865848</td>\n",
       "      <td>6.035484</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.018587</td>\n",
       "      <td>59.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>PERSONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>raynald.westerling@jnj.com</td>\n",
       "      <td>3427.5</td>\n",
       "      <td>276</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>164.6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>34.42</td>\n",
       "      <td>3</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.620404</td>\n",
       "      <td>5.103524</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>2.639129</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>3.538667</td>\n",
       "      <td>5.159061</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.857143</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.025362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>jnj.com</td>\n",
       "      <td>PROFESSIONAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CROSS_SELL_SUCCESS                             EMAIL  REVENUE  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  CANCELLATIONS_AFTER_NOON  PC_LOGINS  MOBILE_LOGINS  WEEKLY_PLAN  LATE_DELIVERIES   AVG_PREP_VID_TIME  LARGEST_ORDER_SIZE  AVG_MEAN_RATING  TOTAL_PHOTOS_VIEWED  c_LATE_DELIVERIES  c_AVG_MEALS_ORDERED  LOG_TOTAL_MEALS_ORDERED  LOG_AVG_TIME_PER_SITE_VISIT  LOG_CANCELLATIONS_AFTER_NOON  LOG_WEEKLY_PLAN  LOG_c_LATE_DELIVERIES  LOG_AVG_PREP_VID_TIME  LOG_TOTAL_PHOTOS_VIEWED  CONT_W_CS_CAT  TOTAL_LOGINS  CANCEL_PROP  PHOTO_PER_LOGIN  PRO_CAT_LOGIN  UNI_PER_TOT  LAT_DEL_PER_TOTAL_MEALS          DOMAIN   EMAIL_CLASS\n",
       "0                   1       steffon.baratheon@yahoo.com   4920.0                  493                   9                            1                         10                    265.6                         5          5              2            0                 0             137.41                   6         2.894737                  456                  0                    6                 6.200511                     5.581995                      1.609638        -6.907755              -6.907755               4.922976                 6.122495              0             7     0.010142        65.142857       1.428571     0.018256                      0.0       yahoo.com      PERSONAL\n",
       "1                   0           harlon.greyjoy@visa.com   6150.0                  361                   9                            1                          6                    247.0                         2          5              1            0                 0             120.20                   5         2.631579                  680                  0                    5                 5.888881                     5.509392                      0.693647        -6.907755              -6.907755               4.789165                 6.522094              0             6     0.005540       113.333333       1.000000     0.024931                      0.0        visa.com  PROFESSIONAL\n",
       "2                   0            monster@protonmail.com   3435.0                  278                   6                            1                          4                    164.4                         0          6              1            5                 0             127.00                   3         3.684211                  145                  0                    3                 5.627625                     5.102309                     -6.907755         1.609638              -6.907755               4.844195                 4.976741              0             7     0.000000        20.714286       0.571429     0.021583                      0.0  protonmail.com      PERSONAL\n",
       "3                   1  damon.lannister.(lord)@yahoo.com   3330.0                  269                   8                            1                          2                    176.0                         5          5              2            0                 0             129.78                   6         3.157895                  418                  0                    6                 5.594715                     5.170490                      1.609638        -6.907755              -6.907755               4.865848                 6.035484              0             7     0.018587        59.714286       0.285714     0.029740                      0.0       yahoo.com      PERSONAL\n",
       "4                   1        raynald.westerling@jnj.com   3427.5                  276                   7                            1                         10                    164.6                         0          6              1           14                 0              34.42                   3         3.157895                  174                  0                    3                 5.620404                     5.103524                     -6.907755         2.639129              -6.907755               3.538667                 5.159061              0             7     0.000000        24.857143       1.428571     0.025362                      0.0         jnj.com  PROFESSIONAL"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating lists for professional, personal and junk email ids basis the email domain\n",
    "\n",
    "# professional mails\n",
    "mail_prof = [\"mmm.com\", \"amex.com\", \"apple.com\", \"boeing.com\", \"caterpillar.com\",\n",
    "            \"chevron.com\", \"cisco.com\", \"cocacola.com\", \"disney.com\", \"dupont.com\",\n",
    "            \"exxon.com\", \"ge.org\", \"goldmansacs.com\", \"homedepot.com\", \"ibm.com\",\n",
    "            \"intel.com\", \"jnj.com\", \"jpmorgan.com\", \"mcdonalds.com\", \"merck.com\",\n",
    "            \"microsoft.com\", \"nike.com\", \"pfizer.com\", \"pg.com\", \"travelers.com\",\n",
    "            \"unitedtech.com\", \"unitedhealth.com\", \"verizon.com\", \"visa.com\",\n",
    "            \"walmart.com\"]\n",
    "\n",
    "# personal mails\n",
    "mail_pers = [\"gmail.com\", \"yahoo.com\", \"protonmail.com\"]\n",
    "\n",
    "# junk mails\n",
    "mail_junk = [\"me.com\", \"aol.com\", \"hotmail.com\", \"live.com\", \"msn.com\", \"passport.com\"]\n",
    "\n",
    "\n",
    "cross_sell['DOMAIN'] = cross_sell['EMAIL'].str.split('@', expand = True)[1]\n",
    "\n",
    "# Creating the column 'EMAIL_CLASS' to classify mail ids into Professional, Personal or Junk\n",
    "cross_sell['EMAIL_CLASS'] = ''\n",
    "\n",
    "# Iterating over each observation to populate the group in the above column\n",
    "for index, value in cross_sell.iterrows():\n",
    "\n",
    "    if cross_sell.loc[index, 'DOMAIN'] in mail_prof:\n",
    "        cross_sell.loc[index, 'EMAIL_CLASS'] = 'PROFESSIONAL'\n",
    "    elif cross_sell.loc[index, 'DOMAIN'] in mail_pers:\n",
    "        cross_sell.loc[index, 'EMAIL_CLASS'] = 'PERSONAL'\n",
    "    elif cross_sell.loc[index, 'DOMAIN'] in mail_junk:\n",
    "        cross_sell.loc[index, 'EMAIL_CLASS'] = 'JUNK'\n",
    "    else:\n",
    "        cross_sell.loc[index, 'EMAIL_CLASS'] = 'ERROR'\n",
    "\n",
    "# Checking the results \n",
    "cross_sell.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39d12ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>PC_LOGINS</th>\n",
       "      <th>MOBILE_LOGINS</th>\n",
       "      <th>WEEKLY_PLAN</th>\n",
       "      <th>LATE_DELIVERIES</th>\n",
       "      <th>AVG_PREP_VID_TIME</th>\n",
       "      <th>LARGEST_ORDER_SIZE</th>\n",
       "      <th>AVG_MEAN_RATING</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "      <th>c_LATE_DELIVERIES</th>\n",
       "      <th>c_AVG_MEALS_ORDERED</th>\n",
       "      <th>LOG_TOTAL_MEALS_ORDERED</th>\n",
       "      <th>LOG_AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>LOG_CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>LOG_WEEKLY_PLAN</th>\n",
       "      <th>LOG_c_LATE_DELIVERIES</th>\n",
       "      <th>LOG_AVG_PREP_VID_TIME</th>\n",
       "      <th>LOG_TOTAL_PHOTOS_VIEWED</th>\n",
       "      <th>CONT_W_CS_CAT</th>\n",
       "      <th>TOTAL_LOGINS</th>\n",
       "      <th>CANCEL_PROP</th>\n",
       "      <th>PHOTO_PER_LOGIN</th>\n",
       "      <th>PRO_CAT_LOGIN</th>\n",
       "      <th>UNI_PER_TOT</th>\n",
       "      <th>LAT_DEL_PER_TOTAL_MEALS</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>EMAIL_CLASS</th>\n",
       "      <th>JUNK</th>\n",
       "      <th>PERSONAL</th>\n",
       "      <th>PROFESSIONAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>steffon.baratheon@yahoo.com</td>\n",
       "      <td>4920.0</td>\n",
       "      <td>493</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>265.6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137.41</td>\n",
       "      <td>6</td>\n",
       "      <td>2.894737</td>\n",
       "      <td>456</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.200511</td>\n",
       "      <td>5.581995</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.922976</td>\n",
       "      <td>6.122495</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.010142</td>\n",
       "      <td>65.142857</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>PERSONAL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>harlon.greyjoy@visa.com</td>\n",
       "      <td>6150.0</td>\n",
       "      <td>361</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.20</td>\n",
       "      <td>5</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.888881</td>\n",
       "      <td>5.509392</td>\n",
       "      <td>0.693647</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.789165</td>\n",
       "      <td>6.522094</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>113.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>visa.com</td>\n",
       "      <td>PROFESSIONAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>monster@protonmail.com</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>278</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>164.4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>127.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.627625</td>\n",
       "      <td>5.102309</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.844195</td>\n",
       "      <td>4.976741</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.714286</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.021583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>protonmail.com</td>\n",
       "      <td>PERSONAL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>damon.lannister.(lord)@yahoo.com</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>269</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>176.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>129.78</td>\n",
       "      <td>6</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>418</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.594715</td>\n",
       "      <td>5.170490</td>\n",
       "      <td>1.609638</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>4.865848</td>\n",
       "      <td>6.035484</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.018587</td>\n",
       "      <td>59.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>PERSONAL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>raynald.westerling@jnj.com</td>\n",
       "      <td>3427.5</td>\n",
       "      <td>276</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>164.6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>34.42</td>\n",
       "      <td>3</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.620404</td>\n",
       "      <td>5.103524</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>2.639129</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>3.538667</td>\n",
       "      <td>5.159061</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.857143</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.025362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>jnj.com</td>\n",
       "      <td>PROFESSIONAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CROSS_SELL_SUCCESS                             EMAIL  REVENUE  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  CANCELLATIONS_AFTER_NOON  PC_LOGINS  MOBILE_LOGINS  WEEKLY_PLAN  LATE_DELIVERIES   AVG_PREP_VID_TIME  LARGEST_ORDER_SIZE  AVG_MEAN_RATING  TOTAL_PHOTOS_VIEWED  c_LATE_DELIVERIES  c_AVG_MEALS_ORDERED  LOG_TOTAL_MEALS_ORDERED  LOG_AVG_TIME_PER_SITE_VISIT  LOG_CANCELLATIONS_AFTER_NOON  LOG_WEEKLY_PLAN  LOG_c_LATE_DELIVERIES  LOG_AVG_PREP_VID_TIME  LOG_TOTAL_PHOTOS_VIEWED  CONT_W_CS_CAT  TOTAL_LOGINS  CANCEL_PROP  PHOTO_PER_LOGIN  PRO_CAT_LOGIN  UNI_PER_TOT  LAT_DEL_PER_TOTAL_MEALS          DOMAIN   EMAIL_CLASS  JUNK  PERSONAL  PROFESSIONAL\n",
       "0                   1       steffon.baratheon@yahoo.com   4920.0                  493                   9                            1                         10                    265.6                         5          5              2            0                 0             137.41                   6         2.894737                  456                  0                    6                 6.200511                     5.581995                      1.609638        -6.907755              -6.907755               4.922976                 6.122495              0             7     0.010142        65.142857       1.428571     0.018256                      0.0       yahoo.com      PERSONAL     0         1             0\n",
       "1                   0           harlon.greyjoy@visa.com   6150.0                  361                   9                            1                          6                    247.0                         2          5              1            0                 0             120.20                   5         2.631579                  680                  0                    5                 5.888881                     5.509392                      0.693647        -6.907755              -6.907755               4.789165                 6.522094              0             6     0.005540       113.333333       1.000000     0.024931                      0.0        visa.com  PROFESSIONAL     0         0             1\n",
       "2                   0            monster@protonmail.com   3435.0                  278                   6                            1                          4                    164.4                         0          6              1            5                 0             127.00                   3         3.684211                  145                  0                    3                 5.627625                     5.102309                     -6.907755         1.609638              -6.907755               4.844195                 4.976741              0             7     0.000000        20.714286       0.571429     0.021583                      0.0  protonmail.com      PERSONAL     0         1             0\n",
       "3                   1  damon.lannister.(lord)@yahoo.com   3330.0                  269                   8                            1                          2                    176.0                         5          5              2            0                 0             129.78                   6         3.157895                  418                  0                    6                 5.594715                     5.170490                      1.609638        -6.907755              -6.907755               4.865848                 6.035484              0             7     0.018587        59.714286       0.285714     0.029740                      0.0       yahoo.com      PERSONAL     0         1             0\n",
       "4                   1        raynald.westerling@jnj.com   3427.5                  276                   7                            1                         10                    164.6                         0          6              1           14                 0              34.42                   3         3.157895                  174                  0                    3                 5.620404                     5.103524                     -6.907755         2.639129              -6.907755               3.538667                 5.159061              0             7     0.000000        24.857143       1.428571     0.025362                      0.0         jnj.com  PROFESSIONAL     0         0             1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding for email classificaton\n",
    "one_hot_EMAIL_CLASS = pd.get_dummies(cross_sell['EMAIL_CLASS']) #converts everything in one and zeroes\n",
    "\n",
    "cross_sell = cross_sell.join([one_hot_EMAIL_CLASS])\n",
    "\n",
    "cross_sell.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de00693",
   "metadata": {},
   "source": [
    "<h3>Correlation Analysis</h3><br>\n",
    "\n",
    "To understand the correlation of different variables with 'CROSS_SELL_SUCCESS', we look at the pearson correlation of the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b03907a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CROSS_SELL_SUCCESS              1.00\n",
       "PROFESSIONAL                    0.19\n",
       "CANCELLATIONS_AFTER_NOON        0.14\n",
       "LOG_CANCELLATIONS_AFTER_NOON    0.12\n",
       "CANCEL_PROP                     0.10\n",
       "TOTAL_LOGINS                    0.07\n",
       "MOBILE_LOGINS                   0.06\n",
       "PERSONAL                        0.04\n",
       "PC_LOGINS                       0.04\n",
       "UNIQUE_MEALS_PURCH              0.04\n",
       "AVG_TIME_PER_SITE_VISIT         0.03\n",
       "LOG_AVG_TIME_PER_SITE_VISIT     0.03\n",
       "LOG_TOTAL_MEALS_ORDERED         0.02\n",
       "c_LATE_DELIVERIES               0.02\n",
       "LOG_AVG_PREP_VID_TIME           0.02\n",
       "LOG_c_LATE_DELIVERIES           0.02\n",
       "c_AVG_MEALS_ORDERED             0.02\n",
       "LARGEST_ORDER_SIZE              0.02\n",
       "LATE_DELIVERIES                 0.02\n",
       "TOTAL_MEALS_ORDERED             0.01\n",
       "AVG_PREP_VID_TIME               0.01\n",
       "LOG_TOTAL_PHOTOS_VIEWED         0.01\n",
       "CONT_W_CS_CAT                   0.01\n",
       "TOTAL_PHOTOS_VIEWED             0.01\n",
       "PRODUCT_CATEGORIES_VIEWED       0.00\n",
       "CONTACTS_W_CUSTOMER_SERVICE    -0.00\n",
       "PHOTO_PER_LOGIN                 0.00\n",
       "REVENUE                         0.00\n",
       "UNI_PER_TOT                    -0.01\n",
       "LAT_DEL_PER_TOTAL_MEALS        -0.01\n",
       "WEEKLY_PLAN                    -0.01\n",
       "LOG_WEEKLY_PLAN                -0.02\n",
       "PRO_CAT_LOGIN                  -0.02\n",
       "AVG_MEAN_RATING                -0.04\n",
       "JUNK                           -0.28\n",
       "Name: CROSS_SELL_SUCCESS, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking correlation between x-variables and response variable - cross_sell_success\n",
    "df_corr = cross_sell.corr(method = 'pearson').round(decimals = 2)\n",
    "\n",
    "df_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f4fc18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CROSS_SELL_SUCCESS  JUNK\n",
       "0                   1       0.583548\n",
       "1                   1       0.416452\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking value counts for discrete data (email classification)\n",
    "cross_sell[['CROSS_SELL_SUCCESS', 'JUNK']][cross_sell['JUNK'] == 1].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d286181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CROSS_SELL_SUCCESS  PROFESSIONAL\n",
       "1                   1               0.800287\n",
       "0                   1               0.199713\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_sell[['CROSS_SELL_SUCCESS', 'PROFESSIONAL']][cross_sell['PROFESSIONAL'] == 1].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20e26d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CROSS_SELL_SUCCESS  PERSONAL\n",
       "1                   1           0.699187\n",
       "0                   1           0.300813\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_sell[['CROSS_SELL_SUCCESS', 'PERSONAL']][cross_sell['PERSONAL'] == 1].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f7a5b",
   "metadata": {},
   "source": [
    "<i><b>Note:</b></i>In value_counts above, we see there is huge split between success of cross sell and non-success of cross-sell for professional and personal emails. Hence in our model we will try these two and drop 'JUNK' from model testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68aa24",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<h3>Building Logistic Regression in statsmodel using explanatory variables</h3><br>\n",
    "We will now run a logistic regression in statsmodel to understand the significance of the above variables. We will then remove the insignificant variables and only retain the significant ones. \n",
    "\n",
    "<h4>Preparing Explanatory and Response variables</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49f738e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanatory variables\n",
    "x_data = cross_sell.drop(['CROSS_SELL_SUCCESS','EMAIL'],axis = 1)\n",
    "\n",
    "# Response variable\n",
    "cross_sell_target  =  cross_sell.loc[ : , 'CROSS_SELL_SUCCESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24784118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data,\n",
    "                                                    cross_sell_target,\n",
    "                                                    test_size    = 0.25,\n",
    "                                                    random_state = 219,\n",
    "                                                    stratify     = cross_sell_target) #stratifying\n",
    "\n",
    "cross_sell_train = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7214e583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Response Variable Proportions (Training Set)\n",
      "--------------------------------------------\n",
      "1    0.68\n",
      "0    0.32\n",
      "Name: CROSS_SELL_SUCCESS, dtype: float64\n",
      "\n",
      "\n",
      "\n",
      "Response Variable Proportions (Testing Set)\n",
      "--------------------------------------------\n",
      "1    0.68\n",
      "0    0.32\n",
      "Name: CROSS_SELL_SUCCESS, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking train and test y variable split\n",
    "print(f\"\"\"\n",
    "\n",
    "Response Variable Proportions (Training Set)\n",
    "--------------------------------------------\n",
    "{y_train.value_counts(normalize = True).round(decimals = 2)}\n",
    "\n",
    "\n",
    "\n",
    "Response Variable Proportions (Testing Set)\n",
    "--------------------------------------------\n",
    "{y_test.value_counts(normalize = True).round(decimals = 2)}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f5ce651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.611507\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>Logit</td>       <td>Pseudo R-squared:</td>    <td>0.026</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td> <td>CROSS_SELL_SUCCESS</td>       <td>AIC:</td>         <td>1788.3775</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>         <td>2023-03-03 22:16</td>        <td>BIC:</td>         <td>1798.9486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>1459</td>         <td>Log-Likelihood:</td>    <td>-892.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>               <td>1</td>             <td>LL-Null:</td>        <td>-916.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>1457</td>          <td>LLR p-value:</td>    <td>4.2418e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>            <td>1.0000</td>            <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>         <td>5.0000</td>               <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>        <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>    <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>    <td>0.4745</td>  <td>0.0672</td>  <td>7.0591</td> <td>0.0000</td> <td>0.3428</td> <td>0.6063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PROFESSIONAL</th> <td>0.8484</td>  <td>0.1266</td>  <td>6.7011</td> <td>0.0000</td> <td>0.6003</td> <td>1.0966</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "===================================================================\n",
       "Model:              Logit              Pseudo R-squared: 0.026     \n",
       "Dependent Variable: CROSS_SELL_SUCCESS AIC:              1788.3775 \n",
       "Date:               2023-03-03 22:16   BIC:              1798.9486 \n",
       "No. Observations:   1459               Log-Likelihood:   -892.19   \n",
       "Df Model:           1                  LL-Null:          -916.19   \n",
       "Df Residuals:       1457               LLR p-value:      4.2418e-12\n",
       "Converged:          1.0000             Scale:            1.0000    \n",
       "No. Iterations:     5.0000                                         \n",
       "---------------------------------------------------------------------\n",
       "                Coef.    Std.Err.     z      P>|z|    [0.025   0.975]\n",
       "---------------------------------------------------------------------\n",
       "Intercept       0.4745     0.0672   7.0591   0.0000   0.3428   0.6063\n",
       "PROFESSIONAL    0.8484     0.1266   6.7011   0.0000   0.6003   1.0966\n",
       "===================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logistic_small = smf.logit(formula   = \"CROSS_SELL_SUCCESS ~ PROFESSIONAL\",\n",
    "                           data = cross_sell_train)\n",
    "\n",
    "\n",
    "# FITTING the model object\n",
    "results_logistic = logistic_small.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "results_logistic.summary2() # summary2() has AIC and BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fec7c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " REVENUE + \n",
      " TOTAL_MEALS_ORDERED + \n",
      " UNIQUE_MEALS_PURCH + \n",
      " CONTACTS_W_CUSTOMER_SERVICE + \n",
      " PRODUCT_CATEGORIES_VIEWED + \n",
      " AVG_TIME_PER_SITE_VISIT + \n",
      " CANCELLATIONS_AFTER_NOON + \n",
      " PC_LOGINS + \n",
      " MOBILE_LOGINS + \n",
      " WEEKLY_PLAN + \n",
      " LATE_DELIVERIES  + \n",
      " AVG_PREP_VID_TIME + \n",
      " LARGEST_ORDER_SIZE + \n",
      " AVG_MEAN_RATING + \n",
      " TOTAL_PHOTOS_VIEWED + \n",
      " c_LATE_DELIVERIES + \n",
      " c_AVG_MEALS_ORDERED + \n",
      " LOG_TOTAL_MEALS_ORDERED + \n",
      " LOG_AVG_TIME_PER_SITE_VISIT + \n",
      " LOG_CANCELLATIONS_AFTER_NOON + \n",
      " LOG_WEEKLY_PLAN + \n",
      " LOG_c_LATE_DELIVERIES + \n",
      " LOG_AVG_PREP_VID_TIME + \n",
      " LOG_TOTAL_PHOTOS_VIEWED + \n",
      " CONT_W_CS_CAT + \n",
      " TOTAL_LOGINS + \n",
      " CANCEL_PROP + \n",
      " PHOTO_PER_LOGIN + \n",
      " PRO_CAT_LOGIN + \n",
      " UNI_PER_TOT + \n",
      " LAT_DEL_PER_TOTAL_MEALS + \n",
      " DOMAIN + \n",
      " EMAIL_CLASS + \n",
      " JUNK + \n",
      " PERSONAL + \n",
      " PROFESSIONAL + \n"
     ]
    }
   ],
   "source": [
    "for val in x_data:\n",
    "    print(f\" {val} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a39ec5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.615570\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>Logit</td>       <td>Pseudo R-squared:</td>    <td>0.020</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td> <td>CROSS_SELL_SUCCESS</td>       <td>AIC:</td>         <td>1804.2338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>         <td>2023-03-03 22:16</td>        <td>BIC:</td>         <td>1825.3758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>1459</td>         <td>Log-Likelihood:</td>    <td>-898.12</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>               <td>3</td>             <td>LL-Null:</td>        <td>-916.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>1455</td>          <td>LLR p-value:</td>    <td>6.9505e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>            <td>1.0000</td>            <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>         <td>5.0000</td>               <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>              <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                <td>-0.2004</td>  <td>0.2517</td>  <td>-0.7959</td> <td>0.4261</td> <td>-0.6937</td> <td>0.2930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UNIQUE_MEALS_PURCH</th>       <td>0.0438</td>   <td>0.0247</td>  <td>1.7748</td>  <td>0.0759</td> <td>-0.0046</td> <td>0.0922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_TIME_PER_SITE_VISIT</th>  <td>0.0023</td>   <td>0.0012</td>  <td>1.9748</td>  <td>0.0483</td> <td>0.0000</td>  <td>0.0047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATIONS_AFTER_NOON</th> <td>0.1973</td>   <td>0.0390</td>  <td>5.0544</td>  <td>0.0000</td> <td>0.1208</td>  <td>0.2738</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                            Results: Logit\n",
       "=======================================================================\n",
       "Model:                Logit               Pseudo R-squared:  0.020     \n",
       "Dependent Variable:   CROSS_SELL_SUCCESS  AIC:               1804.2338 \n",
       "Date:                 2023-03-03 22:16    BIC:               1825.3758 \n",
       "No. Observations:     1459                Log-Likelihood:    -898.12   \n",
       "Df Model:             3                   LL-Null:           -916.19   \n",
       "Df Residuals:         1455                LLR p-value:       6.9505e-08\n",
       "Converged:            1.0000              Scale:             1.0000    \n",
       "No. Iterations:       5.0000                                           \n",
       "-----------------------------------------------------------------------\n",
       "                          Coef.  Std.Err.    z    P>|z|   [0.025 0.975]\n",
       "-----------------------------------------------------------------------\n",
       "Intercept                -0.2004   0.2517 -0.7959 0.4261 -0.6937 0.2930\n",
       "UNIQUE_MEALS_PURCH        0.0438   0.0247  1.7748 0.0759 -0.0046 0.0922\n",
       "AVG_TIME_PER_SITE_VISIT   0.0023   0.0012  1.9748 0.0483  0.0000 0.0047\n",
       "CANCELLATIONS_AFTER_NOON  0.1973   0.0390  5.0544 0.0000  0.1208 0.2738\n",
       "=======================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Builidng the logistic regression model in statsmodel with variables with significant p-values\n",
    "# instantiating a logistic regression model object\n",
    "logistic_small = smf.logit(formula   = \"\"\"CROSS_SELL_SUCCESS ~  \n",
    "                                                                 UNIQUE_MEALS_PURCH+\n",
    "                                                                  \n",
    "                                                                  AVG_TIME_PER_SITE_VISIT +\n",
    "                                                                  CANCELLATIONS_AFTER_NOON \n",
    "                                                        \n",
    "                                                                 \"\"\",\n",
    "                           data = cross_sell_train)\n",
    "                                  \n",
    "\n",
    "# FITTING the model object\n",
    "results_logistic = logistic_small.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "results_logistic.summary2() # summary2() has AIC and BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3eeed197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.567909\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>Logit</td>       <td>Pseudo R-squared:</td>    <td>0.096</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td> <td>CROSS_SELL_SUCCESS</td>       <td>AIC:</td>         <td>1679.1579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>         <td>2023-03-03 22:16</td>        <td>BIC:</td>         <td>1737.2985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>1459</td>         <td>Log-Likelihood:</td>    <td>-828.58</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>10</td>             <td>LL-Null:</td>        <td>-916.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>1448</td>          <td>LLR p-value:</td>    <td>2.2890e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>            <td>1.0000</td>            <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>         <td>6.0000</td>               <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>               <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                 <td>-1.4598</td>  <td>0.3182</td>  <td>-4.5873</td> <td>0.0000</td> <td>-2.0834</td> <td>-0.8361</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REVENUE</th>                   <td>-0.0001</td>  <td>0.0001</td>  <td>-1.2443</td> <td>0.2134</td> <td>-0.0003</td> <td>0.0001</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TOTAL_MEALS_ORDERED</th>       <td>0.0006</td>   <td>0.0014</td>  <td>0.4555</td>  <td>0.6487</td> <td>-0.0021</td> <td>0.0034</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UNIQUE_MEALS_PURCH</th>        <td>0.0575</td>   <td>0.0265</td>  <td>2.1709</td>  <td>0.0299</td> <td>0.0056</td>  <td>0.1094</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PRODUCT_CATEGORIES_VIEWED</th> <td>0.3015</td>   <td>0.0856</td>  <td>3.5213</td>  <td>0.0004</td> <td>0.1337</td>  <td>0.4693</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_TIME_PER_SITE_VISIT</th>   <td>0.0033</td>   <td>0.0017</td>  <td>1.9527</td>  <td>0.0509</td> <td>-0.0000</td> <td>0.0066</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATIONS_AFTER_NOON</th>  <td>0.2173</td>   <td>0.0411</td>  <td>5.2825</td>  <td>0.0000</td> <td>0.1367</td>  <td>0.2980</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>c_LATE_DELIVERIES</th>         <td>0.0140</td>   <td>0.0161</td>  <td>0.8731</td>  <td>0.3826</td> <td>-0.0175</td> <td>0.0456</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PRO_CAT_LOGIN</th>             <td>-2.0935</td>  <td>0.5603</td>  <td>-3.7366</td> <td>0.0002</td> <td>-3.1916</td> <td>-0.9954</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PERSONAL</th>                  <td>1.2766</td>   <td>0.1518</td>  <td>8.4115</td>  <td>0.0000</td> <td>0.9792</td>  <td>1.5741</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PROFESSIONAL</th>              <td>1.7539</td>   <td>0.1653</td>  <td>10.6114</td> <td>0.0000</td> <td>1.4300</td>  <td>2.0779</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                             Results: Logit\n",
       "=========================================================================\n",
       "Model:                Logit                Pseudo R-squared:   0.096     \n",
       "Dependent Variable:   CROSS_SELL_SUCCESS   AIC:                1679.1579 \n",
       "Date:                 2023-03-03 22:16     BIC:                1737.2985 \n",
       "No. Observations:     1459                 Log-Likelihood:     -828.58   \n",
       "Df Model:             10                   LL-Null:            -916.19   \n",
       "Df Residuals:         1448                 LLR p-value:        2.2890e-32\n",
       "Converged:            1.0000               Scale:              1.0000    \n",
       "No. Iterations:       6.0000                                             \n",
       "-------------------------------------------------------------------------\n",
       "                           Coef.  Std.Err.    z    P>|z|   [0.025  0.975]\n",
       "-------------------------------------------------------------------------\n",
       "Intercept                 -1.4598   0.3182 -4.5873 0.0000 -2.0834 -0.8361\n",
       "REVENUE                   -0.0001   0.0001 -1.2443 0.2134 -0.0003  0.0001\n",
       "TOTAL_MEALS_ORDERED        0.0006   0.0014  0.4555 0.6487 -0.0021  0.0034\n",
       "UNIQUE_MEALS_PURCH         0.0575   0.0265  2.1709 0.0299  0.0056  0.1094\n",
       "PRODUCT_CATEGORIES_VIEWED  0.3015   0.0856  3.5213 0.0004  0.1337  0.4693\n",
       "AVG_TIME_PER_SITE_VISIT    0.0033   0.0017  1.9527 0.0509 -0.0000  0.0066\n",
       "CANCELLATIONS_AFTER_NOON   0.2173   0.0411  5.2825 0.0000  0.1367  0.2980\n",
       "c_LATE_DELIVERIES          0.0140   0.0161  0.8731 0.3826 -0.0175  0.0456\n",
       "PRO_CAT_LOGIN             -2.0935   0.5603 -3.7366 0.0002 -3.1916 -0.9954\n",
       "PERSONAL                   1.2766   0.1518  8.4115 0.0000  0.9792  1.5741\n",
       "PROFESSIONAL               1.7539   0.1653 10.6114 0.0000  1.4300  2.0779\n",
       "=========================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Builidng the logistic regression model in statsmodel with variables with significant p-values\n",
    "# instantiating a logistic regression model object\n",
    "logistic_small = smf.logit(formula   = \"\"\"CROSS_SELL_SUCCESS ~  REVENUE + \n",
    "                                                                 TOTAL_MEALS_ORDERED + \n",
    "                                                                 UNIQUE_MEALS_PURCH + \n",
    "                                                               \n",
    "                                                                 PRODUCT_CATEGORIES_VIEWED + \n",
    "                                                                 AVG_TIME_PER_SITE_VISIT + \n",
    "                                                                 CANCELLATIONS_AFTER_NOON + \n",
    "                                                                 c_LATE_DELIVERIES + \n",
    "                                                                  \n",
    "                                                               \n",
    "                                                                 PRO_CAT_LOGIN + \n",
    "                                                                \n",
    "                                                                 PERSONAL + \n",
    "                                                                 PROFESSIONAL  \n",
    "\n",
    "                                                        \n",
    "                                                                 \"\"\",\n",
    "                           data = cross_sell_train)\n",
    "                                  \n",
    "\n",
    "# FITTING the model object\n",
    "results_logistic = logistic_small.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "results_logistic.summary2() # summary2() has AIC and BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d637ad6e",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<h3>Feature Importance</h3><br>\n",
    "\n",
    "We will now look at feature importance for one of the classifiers to get a directional understanding which feature might be important for classification. Here we will use the function taught in class to find important feature for the Cart Model.\n",
    "\n",
    "Here we look at feature importance for gradient boosting classifier. We run the classifier first and then plot the feature importance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1008d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = x_train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Tree_Leaf_50_Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d41fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the data\n",
    "\n",
    "x_data = cross_sell.drop(['CROSS_SELL_SUCCESS','EMAIL', 'DOMAIN', 'EMAIL_CLASS'], axis =1)\n",
    "y_data = cross_sell.loc[:,'CROSS_SELL_SUCCESS']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x_data,\n",
    "            y_data,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "060a1c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Tree Training ACCURACY: 0.7971\n",
      "Full Tree Testing ACCURACY : 0.7166\n",
      "Full Tree Train Test GAP:  0.0805\n",
      "Full Tree AUC Score: 0.6107\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# FITTING the training data\n",
    "model_fit = model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "model_pred = model_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', model_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', model_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "gap = model_fit.score(x_train,y_train).round(4) - model_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Full Tree Train Test GAP: ', gap.round(4) )\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = model_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "model_train_score = model_fit.score(x_train, y_train).round(4) # accuracy\n",
    "model_test_score  = model_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "model_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = model_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f10d044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature_importances(model, train = x_data, export = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c8a30",
   "metadata": {},
   "source": [
    "As per the above feature importance for <b>Gradient Boosting Classifier</b> CART model we have the following features that might be important:\n",
    "\n",
    "1. JUNK\n",
    "2. CANCEL_PROP\n",
    "3. LOG_CANCELLATIONS_AFTER_NOON\n",
    "4. AVG_MEAN_RATING\n",
    "5. REVENUE\n",
    "6. UNIQUE_MEALS_PURCHASED\n",
    "7. PROFESSIONAL\n",
    "8. LAT_DEL_PER_TOT_MEALS\n",
    "9. UNI_PER_TOTAL\n",
    "10. PHOTO_PER_LOGIN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0ede1",
   "metadata": {},
   "source": [
    "Below we have created a dictionary of explanatory variables which can have an impact on the response variable. The sets of explanatory variables are basis significant variable that we arrived at in statsmodel and variables that we tried and tested through feature importance graph and using in different models. \n",
    "\n",
    "<i><b>Note:</b></i> We have run the models multiple using different variables sets as well to arrive at the below list of features and hence the below list is not solely on the statsmodel and feature importance plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de90d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary to store candidate models\n",
    "\n",
    "candidate_dict = {\n",
    "\n",
    " \n",
    " # significant variables only \n",
    " 'set_1'    : ['REVENUE', 'TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH', 'PRODUCT_CATEGORIES_VIEWED', \n",
    "               'AVG_TIME_PER_SITE_VISIT', 'CANCELLATIONS_AFTER_NOON', 'c_LATE_DELIVERIES',\n",
    "                   'PRO_CAT_LOGIN', 'PERSONAL' ,'PROFESSIONAL'],\n",
    "    \n",
    "    \n",
    " 'set_2'  : ['JUNK','CANCELLATIONS_AFTER_NOON','AVG_MEAN_RATING',\n",
    "              'AVG_TIME_PER_SITE_VISIT', 'UNI_PER_TOT','LAT_DEL_PER_TOTAL_MEALS'],\n",
    "    \n",
    " 'set_3' : ['CANCELLATIONS_AFTER_NOON', 'TOTAL_LOGINS', 'PERSONAL', 'PROFESSIONAL'],\n",
    "    \n",
    " 'set_4' :['JUNK', 'CANCEL_PROP', 'LOG_CANCELLATIONS_AFTER_NOON', 'AVG_MEAN_RATING', 'REVENUE',\n",
    "            'UNIQUE_MEALS_PURCH' , 'PROFESSIONAL' , 'LAT_DEL_PER_TOTAL_MEALS' , 'UNI_PER_TOT', 'PHOTO_PER_LOGIN']   \n",
    "\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f746d31",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<h3>Classification Modeling</h3><br>\n",
    "Trying the below models for the above set of candidate variables\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Classifier\n",
    "3. Random Forest Classifier\n",
    "4. Gradient Boosting Classifier\n",
    "\n",
    "We will choose the best combination of explanatory variables and classification model to arrive at the model we will tune and make the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3d211",
   "metadata": {},
   "source": [
    "<h4>Running the Set - 1 of explanatory variables</h4>\n",
    "<br>\n",
    "\n",
    "Running the first set of explanatory variables:<br>\n",
    "\n",
    "'REVENUE', 'TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH', 'PRODUCT_CATEGORIES_VIEWED',               'AVG_TIME_PER_SITE_VISIT', 'CANCELLATIONS_AFTER_NOON', 'c_LATE_DELIVERIES', 'PRO_CAT_LOGIN', 'PERSONAL' ,'PROFESSIONAL'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f42b3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "cross_sell_data   =  cross_sell.loc[ : , candidate_dict['set_1']]\n",
    "cross_sell_target =  cross_sell.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            cross_sell_data,\n",
    "            cross_sell_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = cross_sell_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47c6131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.6779\n",
      "Testing  ACCURACY: 0.6797\n",
      "AUC Score        : 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1, \n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', logreg_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', logreg_fit.score(x_test , y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = logreg_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(x_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(x_test , y_test).round(4) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea665493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Training ACCURACY: 1.0\n",
      "Decision Tree Classifier Testing ACCURACY : 0.6016\n",
      "Decision Tree Classifier AUC Score: 0.551\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "dec_tree_fit = dec_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "dec_tree_pred = dec_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Decision Tree Classifier Training ACCURACY:', dec_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Decision Tree Classifier Testing ACCURACY :', dec_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Decision Tree Classifier AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = dec_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "Dec_tree_train_score_set_1 = dec_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "Dec_tree_test_score_set_1  = dec_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "Dec_tree_auc_score_set_1   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = dec_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e22c254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training ACCURACY: 1.0\n",
      "Random Forest Testing ACCURACY : 0.7064\n",
      "Random Forest Train Test GAP:  0.2936\n",
      "Random Forest AUC Score: 0.5993\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "random_for = RandomForestClassifier()                 \n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "random_for_fit = random_for.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "random_for_pred = random_for_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Random Forest Training ACCURACY:', random_for_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Random Forest Testing ACCURACY :', random_for_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "gap = random_for_fit.score(x_train,y_train).round(4) - random_for_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Random Forest Train Test GAP: ', gap.round(4) )\n",
    "\n",
    "print('Random Forest AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = random_for_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "random_for_train_score_set_1 = random_for_fit.score(x_train, y_train).round(4) # accuracy\n",
    "random_for_test_score_set_1  = random_for_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "random_for_auc_score_set_1   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = random_for_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc11ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Training ACCURACY: 0.7937\n",
      "Gradient Boosting Testing ACCURACY : 0.7125\n",
      "Gradient Boosting Train Test GAP:  0.0812\n",
      "Gradient Boosting AUC Score: 0.6123\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "grad_b = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "grad_b_fit = grad_b.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "grad_b_pred = grad_b_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Gradient Boosting Training ACCURACY:', grad_b_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Gradient Boosting Testing ACCURACY :', grad_b_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "gap = grad_b_fit.score(x_train,y_train).round(4) - grad_b_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Gradient Boosting Train Test GAP: ', gap.round(4) )\n",
    "\n",
    "print('Gradient Boosting AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = grad_b_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "grad_b_train_score_set_1 = grad_b_fit.score(x_train, y_train).round(4) # accuracy\n",
    "grad_b_test_score_set_1  = grad_b_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "grad_b_auc_score_set_1   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = grad_b_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0adc163",
   "metadata": {},
   "source": [
    "<h4>Running all models with Set_2 variables</h4><br>\n",
    "Running the second set of explanatory variables:<br>\n",
    "'JUNK','CANCELLATIONS_AFTER_NOON','AVG_MEAN_RATING',\n",
    " 'AVG_TIME_PER_SITE_VISIT', 'UNI_PER_TOT','LAT_DEL_PER_TOTAL_MEALS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "886d52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "cross_sell_data   =  cross_sell.loc[ : , candidate_dict['set_2']]\n",
    "cross_sell_target =  cross_sell.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            cross_sell_data,\n",
    "            cross_sell_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = cross_sell_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "706eef3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7217\n",
      "Testing  ACCURACY: 0.729\n",
      "AUC Score        : 0.6261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1, \n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', logreg_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', logreg_fit.score(x_test , y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = logreg_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score_set_2 = logreg_fit.score(x_train, y_train).round(4) # accuracy\n",
    "logreg_test_score_set_2  = logreg_fit.score(x_test , y_test).round(4) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67ceddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Training ACCURACY: 1.0\n",
      "Decision Tree Classifier Testing ACCURACY : 0.6078\n",
      "Decision Tree Classifier AUC Score: 0.5505\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "dec_tree_fit = dec_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "dec_tree_pred = dec_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Decision Tree Classifier Training ACCURACY:', dec_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Decision Tree Classifier Testing ACCURACY :', dec_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Decision Tree Classifier AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = dec_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "Dec_tree_train_score_set_2 = dec_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "Dec_tree_test_score_set_2 = dec_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "Dec_tree_auc_score_set_2  = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = dec_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b82ca05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training ACCURACY: 1.0\n",
      "Random Forest Testing ACCURACY : 0.6961\n",
      "Random Forest Train Test GAP:  0.3039\n",
      "Random Forest AUC Score: 0.6002\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "random_for = RandomForestClassifier()                 \n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "random_for_fit = random_for.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "random_for_pred = random_for_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Random Forest Training ACCURACY:', random_for_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Random Forest Testing ACCURACY :', random_for_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "gap = random_for_fit.score(x_train,y_train).round(4) - random_for_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Random Forest Train Test GAP: ', gap.round(4) )\n",
    "\n",
    "print('Random Forest AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = random_for_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "random_for_train_score_set_2 = random_for_fit.score(x_train, y_train).round(4) # accuracy\n",
    "random_for_test_score_set_2  = random_for_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "random_for_auc_score_set_2  = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = random_for_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54cb3ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Training ACCURACY: 0.782\n",
      "Gradient Boosting Testing ACCURACY : 0.7187\n",
      "Gradient Boosting Train Test GAP:  0.0633\n",
      "Gradient Boosting AUC Score: 0.6151\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "grad_b = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "grad_b_fit = grad_b.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "grad_b_pred = grad_b_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Gradient Boosting Training ACCURACY:', grad_b_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Gradient Boosting Testing ACCURACY :', grad_b_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "gap = grad_b_fit.score(x_train,y_train).round(4) - grad_b_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Gradient Boosting Train Test GAP: ', gap.round(4) )\n",
    "\n",
    "print('Gradient Boosting AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = grad_b_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "grad_b_train_score_set_2 = grad_b_fit.score(x_train, y_train).round(4) # accuracy\n",
    "grad_b_test_score_set_2  = grad_b_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "grad_b_auc_score_set_2  = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = grad_b_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b5eb0",
   "metadata": {},
   "source": [
    "<h4>Running Set_3 of the explanatory variables</h4><br>\n",
    "\n",
    "Running the third set of explanatory variables:<br>\n",
    "'CANCELLATIONS_AFTER_NOON', 'TOTAL_LOGINS', 'PERSONAL', 'PROFESSIONAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbced486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "cross_sell_data   =  cross_sell.loc[ : , candidate_dict['set_3']]\n",
    "cross_sell_target =  cross_sell.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            cross_sell_data,\n",
    "            cross_sell_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = cross_sell_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9c464b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7224\n",
      "Testing  ACCURACY: 0.7207\n",
      "AUC Score        : 0.6166\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1, \n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', logreg_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', logreg_fit.score(x_test , y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = logreg_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(x_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(x_test , y_test).round(4) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a03117b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Training ACCURACY: 0.7382\n",
      "Decision Tree Classifier Testing ACCURACY : 0.7207\n",
      "Decision Tree Classifier AUC Score: 0.6285\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "dec_tree_fit = dec_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "dec_tree_pred = dec_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Decision Tree Classifier Training ACCURACY:', dec_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Decision Tree Classifier Testing ACCURACY :', dec_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Decision Tree Classifier AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = dec_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "Dec_tree_train_score_set_3 = dec_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "Dec_tree_test_score_set_3  = dec_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "Dec_tree_auc_score_set_3  = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = dec_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "853dd938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training ACCURACY: 0.7382\n",
      "Random Forest Testing ACCURACY : 0.7228\n",
      "Random Forest Train Test GAP:  0.0154\n",
      "Random Forest AUC Score: 0.6266\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "random_for = RandomForestClassifier()                 \n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "random_for_fit = random_for.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "random_for_pred = random_for_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Random Forest Training ACCURACY:', random_for_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Random Forest Testing ACCURACY :', random_for_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "gap = random_for_fit.score(x_train,y_train).round(4) - random_for_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Random Forest Train Test GAP: ', gap.round(4) )\n",
    "\n",
    "print('Random Forest AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = random_for_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "random_for_train_score_set_3 = random_for_fit.score(x_train, y_train).round(4) # accuracy\n",
    "random_for_test_score_set_3  = random_for_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "random_for_auc_score_set_3  = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = random_for_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32e53fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Training ACCURACY: 0.7341\n",
      "Gradient Boosting Testing ACCURACY : 0.729\n",
      "Gradient Boosting Train Test GAP:  0.0051\n",
      "Gradient Boosting AUC Score: 0.6295\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "grad_b = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "grad_b_fit = grad_b.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "grad_b_pred = grad_b_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Gradient Boosting Training ACCURACY:', grad_b_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Gradient Boosting Testing ACCURACY :', grad_b_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "gap = grad_b_fit.score(x_train,y_train).round(4) - grad_b_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Gradient Boosting Train Test GAP: ', gap.round(4) )\n",
    "\n",
    "print('Gradient Boosting AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = grad_b_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "grad_b_train_score_set_3 = grad_b_fit.score(x_train, y_train).round(4) # accuracy\n",
    "grad_b_test_score_set_3  = grad_b_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "grad_b_auc_score_set_3  = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = grad_b_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c868881",
   "metadata": {},
   "source": [
    "<h4>Running Set-4 of explanatory variables</h4><br>\n",
    "\n",
    "Running the fourth set of explanatory variables:<br>\n",
    "'JUNK', 'CANCEL_PROP', 'LOG_CANCELLATIONS_AFTER_NOON', 'AVG_MEAN_RATING', 'REVENUE',\n",
    "'UNIQUE_MEALS_PURCH' , 'PROFESSIONAL' , 'LAT_DEL_PER_TOTAL_MEALS' , 'UNI_PER_TOT', 'PHOTO_PER_LOGIN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05a220fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "cross_sell_data   =  cross_sell.loc[ : , candidate_dict['set_4']]\n",
    "cross_sell_target =  cross_sell.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            cross_sell_data,\n",
    "            cross_sell_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = cross_sell_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2a64d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7142\n",
      "Testing  ACCURACY: 0.7064\n",
      "AUC Score        : 0.5976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1, \n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', logreg_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', logreg_fit.score(x_test , y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = logreg_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(x_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(x_test , y_test).round(4) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5489e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Training ACCURACY: 1.0\n",
      "Decision Tree Classifier Testing ACCURACY : 0.6057\n",
      "Decision Tree Classifier AUC Score: 0.549\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "dec_tree_fit = dec_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "dec_tree_pred = dec_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Decision Tree Classifier Training ACCURACY:', dec_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Decision Tree Classifier Testing ACCURACY :', dec_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Decision Tree Classifier AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = dec_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "Dec_tree_train_score_set_4 = dec_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "Dec_tree_test_score_set_4  = dec_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "Dec_tree_auc_score_set_4  = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = dec_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "751468be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training ACCURACY: 1.0\n",
      "Random Forest Testing ACCURACY : 0.6961\n",
      "Random Forest Train Test GAP:  0.3039\n",
      "Random Forest AUC Score: 0.5951\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "random_for = RandomForestClassifier()                 \n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "random_for_fit = random_for.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "random_for_pred = random_for_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Random Forest Training ACCURACY:', random_for_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Random Forest Testing ACCURACY :', random_for_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "gap = random_for_fit.score(x_train,y_train).round(4) - random_for_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Random Forest Train Test GAP: ', gap.round(4) )\n",
    "\n",
    "print('Random Forest AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = random_for_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "random_for_train_score_set_4 = random_for_fit.score(x_train, y_train).round(4) # accuracy\n",
    "random_for_test_score_set_4  = random_for_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "random_for_auc_score_set_4  = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = random_for_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0e774a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Training ACCURACY: 0.7848\n",
      "Gradient Boosting Testing ACCURACY : 0.7187\n",
      "Gradient Boosting Train Test GAP:  0.0661\n",
      "Gradient Boosting AUC Score: 0.6185\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "grad_b = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "grad_b_fit = grad_b.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "grad_b_pred = grad_b_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Gradient Boosting Training ACCURACY:', grad_b_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Gradient Boosting Testing ACCURACY :', grad_b_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "gap = grad_b_fit.score(x_train,y_train).round(4) - grad_b_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Gradient Boosting Train Test GAP: ', gap.round(4) )\n",
    "\n",
    "print('Gradient Boosting AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = grad_b_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "grad_b_train_score_set_4 = grad_b_fit.score(x_train, y_train).round(4) # accuracy\n",
    "grad_b_test_score_set_4  = grad_b_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "grad_b_auc_score_set_4  = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = grad_b_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75667944",
   "metadata": {},
   "source": [
    "The explanatory variables 'CANCELLATIONS_AFTER_NOON', 'TOTAL_LOGINS', 'PERSONAL', 'PROFESSIONAL' gives us the best AUC score. We will goahead with these variables. We will hyper parameter tune Decision Tree Classifier, Random Forest Classifier and  Gradient boosting classifier as they have given a good AUC score. \n",
    "\n",
    "<h3>Hyperparameter Tuning</h3><br>\n",
    "We will now try hyper parameter tuning to see if the model performance can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34329f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-declaring the final variables\n",
    "\n",
    "# train/test split with the full model\n",
    "cross_sell_data   =  cross_sell.loc[ : ,['CANCELLATIONS_AFTER_NOON', 'TOTAL_LOGINS', 'PERSONAL', 'PROFESSIONAL']]\n",
    "cross_sell_target =  cross_sell.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            cross_sell_data,\n",
    "            cross_sell_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = cross_sell_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79002344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n",
      "\n",
      "class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
      " |  DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      " |  \n",
      " |  A decision tree classifier.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |  \n",
      " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |          - If int, then consider `max_features` features at each split.\n",
      " |          - If float, then `max_features` is a fraction and\n",
      " |            `int(max_features * n_features)` features are considered at each\n",
      " |            split.\n",
      " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |          - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the randomness of the estimator. The features are always\n",
      " |      randomly permuted at each split, even if ``splitter`` is set to\n",
      " |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      " |      select ``max_features`` at random at each split before finding the best\n",
      " |      split among them. But the best found split may vary across different\n",
      " |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      " |      improvement of the criterion is identical for several splits and one\n",
      " |      split has to be selected at random. To obtain a deterministic behaviour\n",
      " |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  class_weight : dict, list of dict or \"balanced\", default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If None, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      " |      The classes labels (single output problem),\n",
      " |      or a list of arrays of class labels (multi-output problem).\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance [4]_.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_classes_ : int or list of int\n",
      " |      The number of classes (for single output problems),\n",
      " |      or a list containing the number of classes for each\n",
      " |      output (for multi-output problems).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |         `n_features_` is deprecated in 1.0 and will be removed in\n",
      " |         1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree instance\n",
      " |      The underlying Tree object. Please refer to\n",
      " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      " |      for basic usage of these attributes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor : A decision tree regressor.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
      " |  function on the outputs of :meth:`predict_proba`. This means that in\n",
      " |  case the highest predicted probabilities are tied, the classifier will\n",
      " |  predict the tied class with the lowest index in :term:`classes_`.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
      " |  >>> iris = load_iris()\n",
      " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      " |  ...                             # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseDecisionTree\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      " |      Build a decision tree classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      " |          This parameter is deprecated and has no effect.\n",
      " |          It will be removed in 1.1 (renaming of 0.26).\n",
      " |      \n",
      " |          .. deprecated:: 0.24\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : DecisionTreeClassifier\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities of the input samples X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X, check_input=True)\n",
      " |      Predict class probabilities of the input samples X.\n",
      " |      \n",
      " |      The predicted class probability is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Return the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples,)\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      " |      \n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      " |      process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      " |          Dictionary-like object, with the following attributes.\n",
      " |      \n",
      " |          ccp_alphas : ndarray\n",
      " |              Effective alphas of subtree during pruning.\n",
      " |      \n",
      " |          impurities : ndarray\n",
      " |              Sum of the impurities of the subtree leaves for the\n",
      " |              corresponding alpha value in ``ccp_alphas``.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator CSR matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  get_depth(self)\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      The depth of a tree is the maximum distance between the root\n",
      " |      and any leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.max_depth : int\n",
      " |          The maximum depth of the tree.\n",
      " |  \n",
      " |  get_n_leaves(self)\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.n_leaves : int\n",
      " |          Number of leaves.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          Normalized total reduction of criteria by feature\n",
      " |          (Gini importance).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f1b02e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `round(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      Only available if bootstrap=True.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeClassifier\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      " |          removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
      " |      tree classifiers.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |      \n",
      " |      Number of features when fitting the estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c0fe716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GradientBoostingClassifier in module sklearn.ensemble._gb:\n",
      "\n",
      "class GradientBoostingClassifier(sklearn.base.ClassifierMixin, BaseGradientBoosting)\n",
      " |  GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |  \n",
      " |  Gradient Boosting for classification.\n",
      " |  \n",
      " |  GB builds an additive model in a\n",
      " |  forward stage-wise fashion; it allows for the optimization of\n",
      " |  arbitrary differentiable loss functions. In each stage ``n_classes_``\n",
      " |  regression trees are fit on the negative gradient of the\n",
      " |  binomial or multinomial deviance loss function. Binary classification\n",
      " |  is a special case where only a single regression tree is induced.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : {'deviance', 'exponential'}, default='deviance'\n",
      " |      The loss function to be optimized. 'deviance' refers to\n",
      " |      deviance (= logistic regression) for classification\n",
      " |      with probabilistic outputs. For loss 'exponential' gradient\n",
      " |      boosting recovers the AdaBoost algorithm.\n",
      " |  \n",
      " |  learning_rate : float, default=0.1\n",
      " |      Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      " |      There is a trade-off between learning_rate and n_estimators.\n",
      " |  \n",
      " |  n_estimators : int, default=100\n",
      " |      The number of boosting stages to perform. Gradient boosting\n",
      " |      is fairly robust to over-fitting so a large number usually\n",
      " |      results in better performance.\n",
      " |  \n",
      " |  subsample : float, default=1.0\n",
      " |      The fraction of samples to be used for fitting the individual base\n",
      " |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      " |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      " |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |  criterion : {'friedman_mse', 'squared_error', 'mse', 'mae'},             default='friedman_mse'\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are 'friedman_mse' for the mean squared error with improvement\n",
      " |      score by Friedman, 'squared_error' for mean squared error, and 'mae'\n",
      " |      for the mean absolute error. The default value of 'friedman_mse' is\n",
      " |      generally the best as it can provide a better approximation in some\n",
      " |      cases.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |      .. deprecated:: 0.24\n",
      " |          `criterion='mae'` is deprecated and will be removed in version\n",
      " |          1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or\n",
      " |          `'squared_error'` instead, as trees should use a squared error\n",
      " |          criterion in Gradient Boosting.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Criterion 'mse' was deprecated in v1.0 and will be removed in\n",
      " |          version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_depth : int, default=3\n",
      " |      The maximum depth of the individual regression estimators. The maximum\n",
      " |      depth limits the number of nodes in the tree. Tune this parameter\n",
      " |      for best performance; the best value depends on the interaction\n",
      " |      of the input variables.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  init : estimator or 'zero', default=None\n",
      " |      An estimator object that is used to compute the initial predictions.\n",
      " |      ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n",
      " |      'zero', the initial raw predictions are set to zero. By default, a\n",
      " |      ``DummyEstimator`` predicting the classes priors is used.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random seed given to each Tree estimator at each\n",
      " |      boosting iteration.\n",
      " |      In addition, it controls the random permutation of the features at\n",
      " |      each split (see Notes for more details).\n",
      " |      It also controls the random splitting of the training data to obtain a\n",
      " |      validation set if `n_iter_no_change` is not None.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If 'auto', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'sqrt', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'log2', then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Enable verbose output. If 1 then it prints progress and performance\n",
      " |      once in a while (the more trees the lower the frequency). If greater\n",
      " |      than 1 then it prints progress and performance for every tree.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  n_iter_no_change : int, default=None\n",
      " |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      " |      to terminate training when validation score is not improving. By\n",
      " |      default it is set to None to disable early stopping. If set to a\n",
      " |      number, it will set aside ``validation_fraction`` size of the training\n",
      " |      data as validation and terminate training when validation score is not\n",
      " |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      " |      iterations. The split is stratified.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the early stopping. When the loss is not improving\n",
      " |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      " |      number), the training stops.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_estimators_ : int\n",
      " |      The number of estimators as selected by early stopping (if\n",
      " |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      " |      ``n_estimators``.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_improvement_ : ndarray of shape (n_estimators,)\n",
      " |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      " |      relative to the previous iteration.\n",
      " |      ``oob_improvement_[0]`` is the improvement in\n",
      " |      loss of the first stage over the ``init`` estimator.\n",
      " |      Only available if ``subsample < 1.0``\n",
      " |  \n",
      " |  train_score_ : ndarray of shape (n_estimators,)\n",
      " |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      " |      model at iteration ``i`` on the in-bag sample.\n",
      " |      If ``subsample == 1`` this is the deviance on the training data.\n",
      " |  \n",
      " |  loss_ : LossFunction\n",
      " |      The concrete ``LossFunction`` object.\n",
      " |  \n",
      " |  init_ : estimator\n",
      " |      The estimator that provides the initial predictions.\n",
      " |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      " |  \n",
      " |  estimators_ : ndarray of DecisionTreeRegressor of             shape (n_estimators, ``loss_.K``)\n",
      " |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      " |      classification, otherwise n_classes.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of data features.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      " |          removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_classes_ : int\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HistGradientBoostingClassifier : Histogram-based Gradient Boosting\n",
      " |      Classification Tree.\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  RandomForestClassifier : A meta-estimator that fits a number of decision\n",
      " |      tree classifiers on various sub-samples of the dataset and uses\n",
      " |      averaging to improve the predictive accuracy and control over-fitting.\n",
      " |  AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n",
      " |      on the original dataset and then fits additional copies of the\n",
      " |      classifier on the same dataset where the weights of incorrectly\n",
      " |      classified instances are adjusted such that subsequent classifiers\n",
      " |      focus more on difficult cases.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data and\n",
      " |  ``max_features=n_features``, if the improvement of the criterion is\n",
      " |  identical for several splits enumerated during the search of the best\n",
      " |  split. To obtain a deterministic behaviour during fitting,\n",
      " |  ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      " |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      " |  \n",
      " |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      " |  \n",
      " |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      " |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  The following example shows how to fit a gradient boosting classifier with\n",
      " |  100 decision stumps as weak learners.\n",
      " |  \n",
      " |  >>> from sklearn.datasets import make_hastie_10_2\n",
      " |  >>> from sklearn.ensemble import GradientBoostingClassifier\n",
      " |  \n",
      " |  >>> X, y = make_hastie_10_2(random_state=0)\n",
      " |  >>> X_train, X_test = X[:2000], X[2000:]\n",
      " |  >>> y_train, y_test = y[:2000], y[2000:]\n",
      " |  \n",
      " |  >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
      " |  ...     max_depth=1, random_state=0).fit(X_train, y_train)\n",
      " |  >>> clf.score(X_test, y_test)\n",
      " |  0.913...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GradientBoostingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseGradientBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          order of the classes corresponds to that in the attribute\n",
      " |          :term:`classes_`. Regression and binary classification produce an\n",
      " |          array of shape (n_samples,).\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      score : generator of ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |          Regression and binary classification are special cases with\n",
      " |          ``k == 1``, otherwise ``k==n_classes``.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Predict class at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Yields\n",
      " |      -------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the ensemble to X, return leaf indices.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      " |          be converted to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples, n_estimators, n_classes)\n",
      " |          For each datapoint x in X and for each tree in the ensemble,\n",
      " |          return the index of the leaf x ends up in each estimator.\n",
      " |          In the case of binary classification n_classes is 1.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      " |      Fit the gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values (strings or integers in classification, real numbers\n",
      " |          in regression)\n",
      " |          For classification, labels must correspond to classes.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      monitor : callable, default=None\n",
      " |          The monitor is called after each iteration with the current\n",
      " |          iteration, a reference to the estimator and the local variables of\n",
      " |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      " |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      " |          is stopped. The monitor can be used for various things such as\n",
      " |          computing held-out estimates, early stopping, model introspect, and\n",
      " |          snapshoting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb970c2",
   "metadata": {},
   "source": [
    "<h3>Hyperparameter Tuning: Decision Tree Classifier</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b5319f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring hyperparameters for Decision Tree Classifier\n",
    "\n",
    "criterion_range                  = ['gini', 'entropy']                 # criterion\n",
    "splitter_range                   = ['best','random']                   # splitter\n",
    "max_depth_range                  = range(1,8,1)                        # max_depth\n",
    "min_samples_leaf_range           = range(1,10,1)                       # min_samples_leaf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28d6ff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters       : {'splitter': 'random', 'min_samples_leaf': 3, 'max_depth': 6, 'criterion': 'gini'}\n",
      "Tuned Training R-Square: 0.7266\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV # hyperparameter tuning\n",
    "\n",
    "# # creating a hyperparameter grid for decision tree\n",
    "# param_grid = {'criterion'               : criterion_range, \n",
    "#               'splitter'                : splitter_range,  \n",
    "#               'max_depth'               : max_depth_range,\n",
    "#               'min_samples_leaf'        : min_samples_leaf_range}\n",
    "\n",
    "\n",
    "# # INSTANTIATING the Gradient Boosting Regressor model object without hyperparameters\n",
    "# tuned_tree = DecisionTreeClassifier(random_state = 219)\n",
    "\n",
    "# # RandomizedSearchCV object\n",
    "# tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree, # model we are using\n",
    "#                                    param_distributions   = param_grid, # tuning ranges declared above (dictionary)\n",
    "#                                    cv                    = 5,          # how many folds we want \n",
    "#                                    n_iter                = 200,        # max models we want to build \n",
    "#                                    random_state          = 219)        \n",
    "\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# tuned_tree_cv.fit(cross_sell_data, cross_sell_target) \n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters       :\", tuned_tree_cv.best_params_)\n",
    "# print(\"Tuned Training R-Square:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f5ac3",
   "metadata": {},
   "source": [
    "The above parameters give a lower result than the below parameters. We have arrived at the below parameters from trial error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80268566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Training ACCURACY: 0.7204\n",
      "Decision Tree Classifier Testing ACCURACY : 0.7207\n",
      "Decision Tree Classifier AUC Score: 0.6404\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "dec_tree = DecisionTreeClassifier(splitter= 'random', \n",
    "                                  min_samples_leaf= 3, \n",
    "                                  max_depth = 5, \n",
    "                                  criterion= 'gini',\n",
    "                                  random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "dec_tree_fit = dec_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "dec_tree_pred = dec_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Decision Tree Classifier Training ACCURACY:', dec_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Decision Tree Classifier Testing ACCURACY :', dec_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Decision Tree Classifier AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = dec_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "Dec_tree_train_score_set_3 = dec_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "Dec_tree_test_score_set_3  = dec_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "Dec_tree_auc_score_set_3  = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = dec_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223d4f8",
   "metadata": {},
   "source": [
    "<h3>Hyperparameter Tuning: Random Forest Classifier</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d0428f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring hyperparameters for Random Forest Classifier\n",
    "criterion_range             = ['gini', 'entropy']                       # criterion\n",
    "n_estimators_range          = range(50, 150 , 5)                        # n_estimators\n",
    "max_depth_range             = range(1, 8, 1)                            # max_depth \n",
    "min_samples_leaf_range      = range(1,8,1)                              # min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0898ab5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters       : {'n_estimators': 95, 'min_samples_leaf': 3, 'max_depth': 7, 'criterion': 'entropy'}\n",
      "Tuned Training R-Square: 0.7271\n"
     ]
    }
   ],
   "source": [
    "# # creating a hyperparameter grid for forest tree\n",
    "# param_grid = {'criterion'             : criterion_range, \n",
    "#               'n_estimators'     : n_estimators_range,  \n",
    "#               'max_depth'        : max_depth_range,\n",
    "#                'min_samples_leaf': min_samples_leaf_range}\n",
    "\n",
    "\n",
    "# # INSTANTIATING the Gradient Boosting Regressor model object without hyperparameters\n",
    "# tuned_tree = RandomForestClassifier(random_state = 219)\n",
    "\n",
    "# # RandomizedSearchCV object\n",
    "# tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree, # model we are using\n",
    "#                                    param_distributions   = param_grid, # tuning ranges declared above (dictionary)\n",
    "#                                    cv                    = 5,          # how many folds we want \n",
    "#                                    n_iter                = 400,        # max models we want to build \n",
    "#                                    random_state          = 219)        \n",
    "\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# tuned_tree_cv.fit(cross_sell_data, cross_sell_target) \n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters       :\", tuned_tree_cv.best_params_)\n",
    "# print(\"Tuned Training R-Square:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87d8be3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training ACCURACY: 0.7334\n",
      "Random Forest Testing ACCURACY : 0.729\n",
      "Random Forest Train Test GAP:  0.0044\n",
      "Random Forest AUC Score: 0.6244\n"
     ]
    }
   ],
   "source": [
    "#Hypertuned Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "random_for = RandomForestClassifier(criterion         =  'gini', \n",
    "                                    n_estimators      =  145,  \n",
    "                                    max_depth         =  6,\n",
    "                                    min_samples_leaf  =  2,\n",
    "                                   random_state = 219)                 \n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "random_for_fit = random_for.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "random_for_pred = random_for_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Random Forest Training ACCURACY:', random_for_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Random Forest Testing ACCURACY :', random_for_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "gap = random_for_fit.score(x_train,y_train).round(4) - random_for_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Random Forest Train Test GAP: ', gap.round(4) )\n",
    "\n",
    "print('Random Forest AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = random_for_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "random_for_train_score_set_3 = random_for_fit.score(x_train, y_train).round(4) # accuracy\n",
    "random_for_test_score_set_3  = random_for_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "random_for_auc_score_set_3  = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = random_for_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad7650d",
   "metadata": {},
   "source": [
    "<h3>Hyperparameter Tuning: Gradient Boosting Classifier</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dcca8a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring hyperparameters for Gradient Boosting Classifier\n",
    "loss_range                  = ['deviance', 'exponential']               # loss_range\n",
    "n_estimators_range          = range(50, 150 , 5)                        # n_estimators\n",
    "max_depth_range             = range(1, 8, 1)                            # max_depth \n",
    "min_samples_leaf_range      = range(1,8,1)                              # min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f0359a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters       : {'n_estimators': 145, 'min_samples_leaf': 7, 'max_depth': 1, 'loss': 'deviance'}\n",
      "Tuned Training R-Square: 0.7256\n"
     ]
    }
   ],
   "source": [
    "# # creating a hyperparameter grid for classifier tree\n",
    "# param_grid = {'loss'             : loss_range, \n",
    "#               'n_estimators'     : n_estimators_range,  \n",
    "#               'max_depth'        : max_depth_range,\n",
    "#                'min_samples_leaf': min_samples_leaf_range}\n",
    "\n",
    "\n",
    "# # INSTANTIATING the Gradient Boosting classifier model object without hyperparameters\n",
    "# tuned_tree = GradientBoostingClassifier(random_state = 219)\n",
    "\n",
    "# # RandomizedSearchCV object\n",
    "# tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree, # model we are using\n",
    "#                                    param_distributions   = param_grid, # tuning ranges declared above (dictionary)\n",
    "#                                    cv                    = 5,          # how many folds we want \n",
    "#                                    n_iter                = 400,        # max models we want to build \n",
    "#                                    random_state          = 219)        \n",
    "\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# tuned_tree_cv.fit(cross_sell_data, cross_sell_target) \n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters       :\", tuned_tree_cv.best_params_)\n",
    "# print(\"Tuned Training R-Square:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b14b7",
   "metadata": {},
   "source": [
    "These parameters do not give the best AUC score and hence after trial and error below is the model for gradient boosting classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf5039f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Tree Training ACCURACY: 0.732\n",
      "Full Tree Testing ACCURACY : 0.731\n",
      "Full Tree Train Test GAP:  0.001\n",
      "Full Tree AUC Score: 0.631\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "model = GradientBoostingClassifier(    loss               = 'deviance',    \n",
    "                                       max_depth          = 2,              \n",
    "                                       n_estimators       = 80,  \n",
    "                                       min_samples_leaf   = 7,\n",
    "                                       random_state       = 219\n",
    "                                       )                 \n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "model_fit = model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "model_pred = model_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', model_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', model_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "gap = model_fit.score(x_train,y_train).round(4) - model_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Full Tree Train Test GAP: ', gap.round(4) )\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = model_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "model_train_score = model_fit.score(x_train, y_train).round(4) # accuracy\n",
    "model_test_score  = model_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "model_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = model_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a44bbc",
   "metadata": {},
   "source": [
    "<h3><b>Final Model</b></h3><br>\n",
    "The final model that we will go ahead with is <b>Decision Tree Classifier</b>\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<h3>Confusion Matrix</h3>\n",
    "\n",
    "We now plot the confusion matrix for our final model. But before that we run our final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4ea9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Model\n",
    "\n",
    "# train/test split with the full model\n",
    "cross_sell_data   =  cross_sell.loc[ : ,['CANCELLATIONS_AFTER_NOON', 'TOTAL_LOGINS', 'PERSONAL', 'PROFESSIONAL']]\n",
    "cross_sell_target =  cross_sell.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            cross_sell_data,\n",
    "            cross_sell_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = cross_sell_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e1b0603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Training ACCURACY: 0.7204\n",
      "Decision Tree Classifier Testing ACCURACY : 0.7207\n",
      "Decision Tree Classifier Train-Test Gap   : -0.0003\n",
      "Decision Tree Classifier AUC Score        : 0.6404\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "\n",
    "model_name = 'Decision Tree Classifier'\n",
    "\n",
    "dec_tree = DecisionTreeClassifier(splitter= 'random', \n",
    "                                  min_samples_leaf= 3, \n",
    "                                  max_depth = 5, \n",
    "                                  criterion= 'gini',\n",
    "                                  random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "dec_tree_fit = dec_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "dec_tree_pred = dec_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Decision Tree Classifier Training ACCURACY:', dec_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Decision Tree Classifier Testing ACCURACY :', dec_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "dec_gap = dec_tree_fit.score(x_train, y_train).round(4) - dec_tree_fit.score(x_test,y_test).round(4)\n",
    "\n",
    "print('Decision Tree Classifier Train-Test Gap   :', dec_gap.round(4))\n",
    "\n",
    "print('Decision Tree Classifier AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = dec_tree_pred).round(4))\n",
    "\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "Dec_tree_train_score_final = dec_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "Dec_tree_test_score_final = dec_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "Dec_tree_auc_score_final = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = dec_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0b8d431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 65  91]\n",
      " [ 45 286]]\n"
     ]
    }
   ],
   "source": [
    "# Printing the confusion matrix\n",
    "print(confusion_matrix(y_true = y_test,\n",
    "                       y_pred = dec_tree_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b891b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 65\n",
      "False Positives: 91\n",
      "False Negatives: 45\n",
      "True Positives : 286\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "dec_tn, \\\n",
    "dec_fp, \\\n",
    "dec_fn, \\\n",
    "dec_tp = confusion_matrix(y_true = y_test, y_pred = dec_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {dec_tn}\n",
    "False Positives: {dec_fp}\n",
    "False Negatives: {dec_fn}\n",
    "True Positives : {dec_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45100910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calling the visual_cm function\n",
    "visual_cm(true_y = y_test,\n",
    "          pred_y = dec_tree_pred,\n",
    "          labels = ['No Cross Sell Success', 'Cross Sell Success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c42998",
   "metadata": {},
   "source": [
    "<h3>Model Explaination</h3><br>\n",
    "\n",
    "The final Decision Tree Classifier model has a training accuracy of 0.7204 and a testing accuracy of 0.7207, which means that the model is performing well on the unseen data as well.\n",
    "\n",
    "The AUC score of 0.64 indicates that the model has moderate power to differentiate between negative and positive cases. However, lower train-test gap ensures that there is no over fitting that has happened in the model. \n",
    "\n",
    "However, even though the model correctly predicted 351 cases correctly, which is good, it did not predict the others quite well. \n",
    "\n",
    "The model incorrectly predicted 91 false positives, which is not desirable and 45 false negatives.\n",
    " \n",
    "Hence there is scope improvement in the model. \n",
    "\n",
    "For our model:\n",
    "<br><br>\n",
    "\n",
    "~~~\n",
    "                                                 |\n",
    "  PREDICTED: NO CROSS SELL SUCCESS (0)           |  PREDICTED: CROSS SELL SUCCESS    (1)  \n",
    "  ACTUAL:    NO CROSS SELL SUCCESS (0)           |  ACTUAL:    NO CROSS SELL SUCCESS (0) \n",
    "                                                 |\n",
    "-------------------------------------------------|-----------------------------------------------\n",
    "                                                 |\n",
    "  PREDICTED: NO CROSS SELL SUCCESS (0)           |  PREDICTED: CROSS SELL SUCCESS (1)\n",
    "  ACTUAL:    CROSS SELL SUCCESS    (1)           |  ACTUAL:    CROSS SELL SUCCESS (1)\n",
    "                                                 |  \n",
    "~~~\n",
    "\n",
    "\n",
    "Having high false positives like in this case can mislead the company to think that the promotion works very efficiently and there is no need to evaluate anything on the promotion. However, on ground the reality might be that the promotion is working for certain kind of customer and not working for other cohort of customer and may need a change in the promotion itself in terms of attractiveness or might need more marketing to increase awareness about it. \n",
    "\n",
    "A high number of true positives does indicate that the promotion works and in turn might be leading to high sales and revenue for Apprentice chef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14f24a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***************************************************\n",
      "                  FINAL MODEL\n",
      "\n",
      "Model Name        : Decision Tree Classifier\n",
      "           \n",
      "Training Accuracy : 0.7204\n",
      "           \n",
      "Testing Accuracy  : 0.7207\n",
      "\n",
      "Train - Test Gap  : -0.0003\n",
      "\n",
      "AUC Score         : 0.6404\n",
      "\n",
      "Confusion Matrix  : True Negatives : 65\n",
      "                    False Positives: 91\n",
      "                    False Negatives: 45\n",
      "                    True Positives : 286\n",
      "\n",
      "***************************************************\n"
     ]
    }
   ],
   "source": [
    "#Displaying final result\n",
    "\n",
    "# comparing results\n",
    "print(f\"\"\"\n",
    "\n",
    "***************************************************\n",
    "                  FINAL MODEL\n",
    "\n",
    "Model Name        : {model_name}\n",
    "           \n",
    "Training Accuracy : {Dec_tree_train_score_final}\n",
    "           \n",
    "Testing Accuracy  : {Dec_tree_test_score_final}\n",
    "\n",
    "Train - Test Gap  : {dec_gap.round(4)}\n",
    "\n",
    "AUC Score         : {Dec_tree_auc_score_final}\n",
    "\n",
    "Confusion Matrix  : True Negatives : {dec_tn}\n",
    "                    False Positives: {dec_fp}\n",
    "                    False Negatives: {dec_fn}\n",
    "                    True Positives : {dec_tp}\n",
    "\n",
    "***************************************************\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "469.333px",
    "left": "41px",
    "top": "177px",
    "width": "256px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
